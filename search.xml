<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Research Plan]]></title>
    <url>%2F2017%2F06%2F10%2Frs2%2F</url>
    <content type="text"><![CDATA[ObjectiveTo propose a new sequence alignment method based on neural network. ProblemGiven an sequence pair set S. In set S, each sequence pair &lt; ${Seq}_A$, ${Seq}_B$&gt; has best alignment form $Align({Seq}_A,{Seq}_B)$. For example, pair &lt; ${Seq}_A$, ${Seq}_B$&gt;: LDRCVPKFWTLHKN LEKCWTMNMCQKN the best alignment form is: LDRCVPKFWTL——HKN LEKC——WTMNMCQKN For a new sequence pair not in S, how to predict its best alignment form. Further Formalize ProblemWe change this problem to another scenery. There are two queue QueueA and QueueB. SeqA is in QueueA, SeqB is in QueueB. There are three allowed action to be adopted at two queue: M, I,and D. which means,123456M: dequeue QueueA and QueueBI: only dequeue QueueAD: only dequeue QueueB each time adopts a stategy. when two queue is empty, we get a strategy sequence. then problem is change to this statement. Given an sequence pair set S. In set S, each sequence pair &lt; ${Seq}_A$, ${Seq}_B$&gt; has best dequeue strategy process $Align({Seq}_A,{Seq}_B)$. For SeqA and SeqB above, the best dequeue strategy process is: MMMMIIIIMMMMMDDDDMMM For a new sequence pair not in S, how to predict its best strategy process. MotivationThe strength of deep learning is feature extraction, metric space mapping and big data processing. Consider the problem complexity of sequence alignment, only simple deep learning method is not enough. Seq2seq model provides a simple end-to-end architecture to deal with a multi-step decision problem. Furthermore, Seq2seq model can be easily modified and expanded. By formalizing sequence alignment to strategy sequence, we adopt seq2seq neural network to model sequence alignment problem. MethodDataSetSCOPe2.6 | database | proteins | alignments_family | alignments_superfamily | alignments_fold | |:———-:————|:——:——|:————|:————-|:————-| | scope20_2.06 | 7659 | 16194 | 66229 | 133900 | | scope40_2.06 | 13760 | 98558 | 305553 | 526136 | | scope70_2.06 | 21132 | 314039 | 907374 | 1470385 | | scope95_2.06 | 28010 | 1099425 | 2963970 | 4170984 | | scope100_2.06 | 69590 | 6060416 | 16688439 | 23215716 | | scopeall_2.06 | 216248 | 69574992 | 172157027 | 239906177 | Model Textual EncoderGiven two input sequences $X_1=(a_1,a_2,…,a_{T_1}),X_2=(b_1,b_2,…,b_{T_2})$by Bi-LSTM and Multi_LSTM, get encodes of input sequencesIn another perspective, we can consider encoders as memory of input. $A^{X_1}=Encoder(a_1,a_2,…,a_{T_1})=\{h_1^{X_1},h_2^{X_1},…,h_{T1}^{X_1}\}$ $A^{X_2}=Encoder(b_1,b_2,…,b_{T_2})=\{h_1^{X_2},h_2^{X_2},…,h_{T2}^{X_2}\}$ Joint mutimodal representationFor first try, we consider to compute outer product from two input memory to get a joint representation.Compared with element-wise product, a multiplicative interaction between all elements of both vectors can describe more complex relationship. Bilinear pooling.Bilinear models (Tenenbaum and Freeman, 2000) take the outer product of two vectors $x ∈ R^{n_1}$ and $q ∈ R^{n_2}$ and learn a model $W$ (here linear), i.e.$z = W [x ⊗ q]$, where $⊗$ denotes the outer product$(xq^T)$ and $[]$ denotes linearizing the matrix in a vector.However, the high dimensional representation leads to an infeasible number of parameters to learn in $W$.We thus need a method that projects the outer product to a lower dimensional space and also avoids computing the outer product directly. Multimodal Compact Bilinear Pooling (MCB)This allows us to project the outer product to a lower dimensional space, which reduces the number of parameters in W. To avoid computing the outer product explicitly, Pham and Pagh (2013) showed that the count sketch of the outer product of two vectors can be expressed as convolution of both count sketches: $Ψ(x ⊗ q, h, s) = Ψ(x, h, s) ∗ Ψ(q, h, s)$, where ∗ is the convolution operator. Additionally, the convolution theorem states that convolution in the time domain is equivalent to element-wise product in the frequency domain. The convolution $x’∗ q’$ can be rewritten as ${FFT}^{−1}(FFT(x’) \centerdot FFT(q’))$, where $\centerdot$ refers to element-wise product. These ideas are formalized in Algorithm 1, which is based on the Tensor Sketch algorithm of Pham and Pagh (2013). We invoke the algorithm with $A^{X_1} = x$ and $A^{X_2} = q$. We note that this easily extends and remains efficient for more than two multi-modal inputs as the combination happens as element-wise product. A=MCB(A^{X_1},A^{X_2})=\{h_1,h_2,...,h_{T}\}AttentionAt every time-step, the attention mechanism computes a context vector $c^t$, given the current decoder state $s_t$ and memory set $A^m$.Firstly, we compute attention weights $α_t = softmax(v^Ttanh(W_1A + W_2s_t + b))$.Then, the context vector is obtained with the following weighted sum : $c_t =\sum\limits ^{|A|}_{i=1} α_{ti}h_i$ DecoderThe decoder produces an output sequence $Y = (y1, y2, …, y_{T_0} )$, is initialized by $s0 = tanh(W_{init}h^t_T + b_{init})$ where $h^t_T$ is the textual encoder’s last state.The next decoder states are obtained as follows:$s_t, o_t = Decoder(s_{t−1}, W_{in}[y_{t−1}: c_{t−1}])$During training, $y_{t−1}$ is the ground truth symbol in the sentence while $c_{t−1}$ is the previous attention vector computed by the attention model. The current attention vector $c_t$, concatenated with the output $o_t$, is used to compute a new vector $o’_t = W_{proj} [o_t; c_t] + b{proj} $.The probability distribution over the target vocabulary is computed by the equation :$p(y_t|y_{t−1}, s_{t−1}, c_{t−1}, A^{X_1}, A^{X_2}) = softmax(W_{out}o’_t + b_{out})$ Loss FunctionOptimizerResultsConclusion]]></content>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conference]]></title>
    <url>%2F2017%2F06%2F05%2Fconference%2F</url>
    <content type="text"><![CDATA[NIPShttp://www.tuicool.com/articles/uuAjQnv NIPS 2016生成模型highlightGANPlug &amp; Play Generative Networks - Ian Goodfellow VAEVariational Inference- David Blei最有影响的是重新参数化（reparameterization）的技巧，该技巧可以通过随机变量实现反向传播，同时也推动了变自编码器上最新进展。 SAGASAGA VS BFGS - Francis Bachhttps://arxiv.org/abs/1407.0202 othersExponential Family Embeddings一种多类型数据的全新强大的嵌入（embedding）技术，带来了用户使用嵌入技术评估数据的可能性。 Unsupervised Learning for Physical Interaction through Video Prediction利用机器人的推动动作的数据来规划可能的未来。传统的代理（agent）学习算法严重依赖于监督，而这种类型的方法或许是机器人和类似领域未来的新方法。 Improving Variational Inference with Inverse Autoregressive Flow结合了变分推断的最新进展和自动回归网络（autoregressive network）的一些想法，得出更好的编码模型。 强化学习highlightValue Iteration Network该论文的主要创新在于其模型包含了一个可微分的「规划模块（planning module），这让网络可以做出规划并更好地泛化到其它从未见过的领域。 Sequential Neural Models with Stochastic Layers将 状态空间模型（State Space Model）的想法和 RNN 结合起来，充分利用了两个领域的最好的东西。 Phased LSTMs将「time gate」添加到了 LSTM 中，这显著改善了针对长序列数据的优化和表现。 Bayesian Intermittent Demand Forecasting for Large Inventories讨论了针对大型库存的贝叶斯间断需求预测 Fast and Provably Good Seedings for k-MeansK-means 是许多数据科学应用的核心算法。不过，找到好的聚类中心（cluster centers）常常要依赖良好的初始化。Olivier Bachem 在论文《Fast and Provably Good Seedings for k-Means》中表明，他们可以获得良好的 centroid seeds，速度比当前最佳的算法（k-Means++）快几个数量级。更妙的是，他们还有代码，「pip install kmc2」= g2g。 othersAttend, Infer, Repeat: Fast Scene Understanding with Generative Models提出了一种极具启发性的理解图像中场景的方法。使用贝叶斯和变分推理（Bayesian and Variational Inference），该论文的作者构建了一个可以无需任何监督就能理解图像中的数字、地址和物体类型的模型。这引起了较大的关注，因为他们的模型可以在训练样本之外的分布上进行推理/推导。当然，该模型确实需要一些特别的需求，但它们也提供了新的有趣的研究探索路径。 DeepMath—Deep Sequence Models for Premise Selection提出的深度学习方法可以持续不断突破新的领域。一个来自 Google Research 的研究团队（包括 François Chollet and Geoffrey Irving）展示了世界上第一个使用深度学习进行自动理论证明（automated theorem proving）的案例。这项成果有助于加速系统的正确性证明，并可替代对该领域的专家所设计的特征的需求（其与自然语言有类似但也不同的结构）。它们可以自动选择与推理过程中的当前状态相关的操作运算，这个过程可以被扩展到其它领域，是一个非常激动人心的研究方向。 # Supervised Word Mover’s Distance词嵌入帮助改变了许多自然语言处理任务，去年《Word Movers Distance》提供一种使用它们的嵌入在不同文档进行摘要的方法。对监督任务（比如，文本分类）而言，这可以更进一步。《Supervised Word Mover’s Distance》提出了可以执行仿射变换（affine transformation）和重新调整权重（re-weightings）的方法来提供分类，实现了有效的当前最佳的表现。 应用:机器人与汽车Learning to Poke by Poking: Experiential Learning of Intuitive Physics。他们使用了几百个小时数据（让机器人通过戳的动作来移动物体获得的）搭建了一个系统，机器人可以四处移动物体即使它从未见过这些物体。系统使用了 CNNs 来观察世界，有两个理解相关物理世界的模型。前向模型（forward model），用来预测一个动作/戳的结果，以及一个能够获取当前状况并将之映射到行动中的逆模型（inverse model）。通过一系列令人信服的视频，很明显，机器人已经学会如何相当普遍地四处移动物体。 似然推理、Dessert 类比（Dessert analogies）]]></content>
  </entry>
  <entry>
    <title><![CDATA[RAM]]></title>
    <url>%2F2017%2F06%2F05%2FRAM%2F</url>
    <content type="text"><![CDATA[http://jacoxu.com/ramreasoning-attention-memory%E4%B8%8Eqa%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%E8%B5%84%E6%BA%90%E5%88%97%E8%A1%A8/]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLP]]></title>
    <url>%2F2017%2F06%2F05%2FNLP%2F</url>
    <content type="text"><![CDATA[http://www.iteye.com/news/31261]]></content>
  </entry>
  <entry>
    <title><![CDATA[CV]]></title>
    <url>%2F2017%2F06%2F05%2FCV%2F</url>
    <content type="text"><![CDATA[http://blog.csdn.net/Hao_Zhang_Vision/article/details/53175979?locationNum=2&amp;fps=1http://blog.csdn.net/cuixiaoxue/article/details/70138784http://blog.csdn.net/sinat_26917383/article/details/54669846]]></content>
  </entry>
  <entry>
    <title><![CDATA[attention]]></title>
    <url>%2F2017%2F06%2F05%2Fattention%2F</url>
    <content type="text"><![CDATA[http://blog.csdn.net/jdbc/article/details/52948351http://www.cnblogs.com/taojake-ML/p/6113459.html Show, Attend and Tell: Neural Image Caption Generation with Visual Attentionhttp://www.cnblogs.com/wangxiaocvpr/p/5537454.htmlhttp://blog.csdn.net/cuixiaoxue/article/details/70138784Teaching Machines to Read and Comprehendhttp://blog.csdn.net/u014300008/article/details/52763100http://chuansong.me/n/1362687251865]]></content>
  </entry>
  <entry>
    <title><![CDATA[VAE]]></title>
    <url>%2F2017%2F06%2F05%2FVAE%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[GAN]]></title>
    <url>%2F2017%2F06%2F05%2FGAN%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[image-caption]]></title>
    <url>%2F2017%2F06%2F05%2Fimage-caption%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[RNNs]]></title>
    <url>%2F2017%2F06%2F05%2FRNNs%2F</url>
    <content type="text"><![CDATA[LSTM http://colah.github.io/posts/2015-08-Understanding-LSTMs/ Tree-Structured LSTMsA generalization of the standard LSTM architecture to tree structure.Input $x_i$ regards $x_{i-1}$ as its child, then generalize one child to more. Child-Sum Tree-LSTMs UsageSince the Child-Sum Tree-LSTM unit conditions its components on the sum of child hidden states $h_k$ , it is well-suited for trees with high branching factor or whose children are unordered.For example, it is a good choice for dependency trees, where the number of dependents of a head can be highly variable.We refer to a Child-Sum Tree-LSTM applied to a dependency tree as a Dependency Tree-LSTM. N-ary Tree-LSTMs UsageIn Eq. 10, we define a parameterization of the kth child’s forget gate $f_{jk}$ that contains “off-diagonal” parameter matrices $U_{kl}^{(f)}$,$ k \not= l $. This parameterization allows for more flexible control of information propagation from child to parent.We can naturally apply Binary Tree-LSTM units to binarized constituency trees since left and right child nodes are distinguished.We refer to this application of Binary Tree-LSTMs as a Constituency Tree-LSTM.]]></content>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NMT]]></title>
    <url>%2F2017%2F06%2F05%2FNMT%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[QA]]></title>
    <url>%2F2017%2F06%2F05%2FQA%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[VQA]]></title>
    <url>%2F2017%2F06%2F05%2FVQA%2F</url>
    <content type="text"><![CDATA[http://blog.csdn.net/lsh894609937/article/details/70210541http://blog.csdn.net/mxs30443/article/details/53540649http://blog.csdn.net/zdcs/article/details/54944643https://zhuanlan.zhihu.com/p/22530291?refer=dlclass]]></content>
      <tags>
        <tag>VQA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[work_temp]]></title>
    <url>%2F2017%2F06%2F05%2Fwork-temp%2F</url>
    <content type="text"><![CDATA[整理scope配对数据整理从scope数据库找到的连配对。汇总并统计数据。 database proteins alignments_family alignments_superfamily alignments_fold scope20_2.06 7659 16194 66229 133900 scope40_2.06 13760 98558 305553 526136 scope70_2.06 21132 314039 907374 1470385 scope95_2.06 28010 1099425 2963970 4170984 scope100_2.06 69590 6060416 16688439 23215716 scopeall_2.06 216248 69574992 172157027 239906177 Tensorflow 框架下NMT分析google - seq2seqA general-purpose encoder-decoder frameworkused for the paper:Massive Exploration of Neural Machine Translation Architectures eske - seq2seq (baseline branch)replicate “Neural Machine Translation by Jointly Learning to Align and Translate” paarthneekhara - byteNet-tensorflow (need to read)replicate “Neural Machine Translation in Linear Time.” ufal - neuralmonkey NAn open-source tool for sequence learning in NLP built on TensorFlow.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Embeddings in Biological Sequences]]></title>
    <url>%2F2017%2F05%2F22%2Fbiovec%2F</url>
    <content type="text"><![CDATA[ObjectiveConstruct a distributed representation of biological sequences. Related WorkWord2vec CBOW Skip-gram Doc2vecDoc2vec-CBOW Doc2vec-DM Effect In fact, unstable. BioVec: ProtVec+GeneVec （Plos one）ObjectiveOur goal is to construct a distributed representation of biological sequences. MethodHere, a biological sequence is treated like a sentence in a text corpus while the kmers derived from the sequence are treated like words, and are given as input to the embedding algorithm. Data Preprocess NN ModelSkip-gram Model EvaluationProtein Family ClassificationTo evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families,Database：Swiss-ProtMethod: SVMResults：an average family classification accuracy of 93% ± 0.06% is obtained, outperforming existing family classification methods. Disordered Proteins Classification In addition, we use ProtVec representation to predict disordered proteins from structured proteins.Database:The DisProt database as well as FG-Nups (a database featuring thedisordered regions of nucleoporins rich with phenylalanine-glycine repeats).Method:Using support vector machine classifiers,Results:FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0% accuracy. Conclusion By only providing sequence data for various proteins into this model, accurate information about protein structure can be determined. Importantly, this model needs to be trained only once and can then be applied to extract a comprehensive set of information regarding proteins of interest. Moreover, this representation can be considered as pre-training for various applications of deep learning in bioinformatics. Seq2vec (DTMBio)ObjectiveConstructing a distributed representation of biological sequences. MethodOur algorithm is based on the doc2vec approach, which is an extension of the original word2vec algorithm. Data PreprocessTwo approach. Non-overlapping Process QWERTYQWERTY-&gt;Seq 1: QWE RTY QWE RTYSeq 2: WER TYQ WERSeq 3: ERT YQW ERT. Overlapping Process: QWERTYQWERTY-&gt;QWE WER ERT RTY TYQ YQW QWE WER ERT RTY. EvaluationProtein Classification.First, we use the protein vectors learned using seq2vec and ProtVecs as features and compare them for the task of protein classification using SVMs.Next, since distributed representations embed similar sequences in proximity to each other, we use k-nearest neighbors (kNN) to retrieve the k nearest sequences in the vector space and see how successful are we in predicting the family of a test sequence based on a majority vote.]]></content>
      <tags>
        <tag>embedding</tag>
        <tag>biological</tag>
        <tag>sequences</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo usage]]></title>
    <url>%2F2017%2F05%2F19%2Fstart%2F</url>
    <content type="text"><![CDATA[Create a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词向量 word embedding]]></title>
    <url>%2F2017%2F05%2F19%2Fword-embedding%2F</url>
    <content type="text"><![CDATA[Objective问题起源文本是符号数据,两个词只要字面不同,就难以刻画它们之间的联系,即使是“麦克风”和“话筒”这样的同义词,从字面上也难以看出这两者意思相同(语义鸿沟现象)。（符号性这个角度类似于生物序列数据）我们希望计算机可以从大规模无标注的文本数据中自动学习得到文本表示,这种表示需要包含对应语言单元(词或文档)的语义信息,同时可以直接通过这种表示度量文本之间的语义相似度。 形式化问题 1 输入：语句的集合/文章的集合输出: 语言中词的一种表示 1954年，Harris 提出分布假说(distributional hypothesis),即“上下文相似的词,其语义也相似“。（只针对词）在分布假说中,需要关注的对象有两个:词和上下文,其中最关键的是上下文的表示。 形式化问题 2 输入：词的上下文输出: 词的一种表示 神经网络模型可以使用组合方式(?)对上下文进行建模,只需线性复杂度即可对复杂的 n 元短语进行建模。神经网络模型生成的词表示通常被称为词向量(word embedding)。 词向量：一个低维的实数向量表示。通过这种表示,可以直接对词之间的相似度进行刻画。 问题形式化 输入: 词的上下文输出: 词的低维实数向量表示方法: 神经网络 Related Knowledge符号表示$w$ 表示一个词语.$w_1 , w_2 , …, w_m $表示一条由m个词语组成的语句. 语言模型语言模型可以对一段文本的概率进行估计.形式化的讲,统计语言模型的作用是为一个长度为 $m$ 的字符串确定一个概率分布 $P (w_1 , w_2 , …, w_m )$ P(w_1 , w_2 , ..., w_m ) = P (w_1 ) P (w_2 |w_1 ) \dots P (w_i | w_1 , w_2 , ..., w_{i−1} )\dots P (w_m | w_1 , w_2 , ..., w_{m−1} )$n$元(n-gram)模型如果文本的长度较长,公式右部$ P (w_i | w_1 , w_2 , \dots , w_{i−1} ) $的估算会非常困难。因此,研究者们提出使用一个简化模型:$n$ 元模型.对上述条件概率做了以下近似: P (w_i | w_1 , w_2 , ..., w_i−1 ) ≈ P (w_i | w_{i−(n−1)} , . . . , w_i−1 )在 n 元模型中,传统的方法一般采用频率计数的比例来估算 $n$ 元条件概率: P (w_i | w_{i−(n−1)} , . . . , w_{i−1} ) = \frac{count(w_{i−(n−1)} , . . . , w_{i−1} , w_i ) }{count(w_{i−(n−1)} , . . . , w_{i−1} )}其中,$count(w_{i−(n−1)} , . . . , w_{i−1} )$ 表示文本序列 $w_{i−(n−1)} , . . . , w_{i−1}$ 在语料中出现的次数。 MethodMethod 1: 神经网络语言模型(NNLM)RNNLM 利用前n个词的上文信息,预测下一个词.即,直接对$ P (w_i | w_1 , w_2 , …, w_{i−1} ) $进行建模.RNNLM是利用神经网络对条件概率建模. 网络输入输出 目标: 最大化 $P (w_{i-(n-1)/2}| w_{i−(n−1)} , . . . , w_{i−1} )$输入: 整个词序列 $w_{i−(n−1)} , . . . , w_{i−1}$输出: 目标词的分布 网络结构示意图 输入层词$w_{i−(n−1)} , . . . , w_{i−1}$的词向量的顺序拼接 x = [e(w_{i−(n−1)} ); . . . ; e(w_{i−2} ); e(w_{i−1} )]隐藏层hh= tanh(b^{(1)} + Hx)y = b ^{(2)} + W x + U h其中,其中,$φ$ 为非线性激活函数。根据公式,每一个隐藏层包含了当前词的信息以及上一个隐藏层的信息。通过这种迭代推进的方式,每个隐藏层实际上包含了此前所有上文的信息. 输出层 P (w_i | w_{i−(n−1)} , . . . , w_{i−1} ) = \frac {exp (y(w_i))}{\sum_{k=1}^{|V|}exp(y(v_k))}网络目标最大化 $\sum\limits _{w _{i−(n−1):i} ∈D}log P (w_i | w_{i−(n−1)} ,\dots, w_{i−1} )$即最大化语言模型. Method 2: 循环神经网络语言模型(RNNLM)RNNLM 利用所有的上文信息,预测下一个词.即,直接对$ P (w_i | w_1 , w_2 , …, w_{i−1} ) $进行建模.RNNLM是利用神经网络对条件概率建模. 网络输入输出 目标: 最大化 $P (w_i | w_{i−(n−1)} , . . . , w_1 )$输入: 整个词序列 $w_{i−(n−1)} , . . . , w_{i−1}$输出: 目标词的分布 网络结构示意图 输入层词$w_{i−(n−1)} , . . . , w_{i−1}$的词向量的顺序拼接 x = [e(w_{i−(n−1)} ); . . . ; e(w_{i−2} ); e(w_{i−1} )]隐藏层hh(i) = φ(e(w_i ) + w_h(i − 1))y(i) = b + uh(i)其中,其中,$φ$ 为非线性激活函数。根据公式,每一个隐藏层包含了当前词的信息以及上一个隐藏层的信息。通过这种迭代推进的方式,每个隐藏层实际上包含了此前所有上文的信息. 输出层 P (w_i | w_{i−(n−1)} , . . . , w_1 ) = \frac {exp (y(w_i))}{\sum_{k=1}^{|V|}exp(y(v_k))}网络目标最大化语言模型$\sum\limits _{w _{i−(n−1):i} ∈D}log P (w_i | w_{i−(n−1)} ,\dots, w_1 )$ Method 3: CBOW 模型该模型一方面使用一段文本的中间词作为目标词;另一方面,又以 NNLM 作为蓝本,并在其基础上做了两个简化。一、CBOW 没有隐藏层,去掉隐藏层之后,模型从神经网络结构直接转化为 log 线性结构,与 Logistic 回归一致。log 线性结构比三层神经网络结构少了一个矩阵运算,大幅度地提升了模型的训练速度。二、CBOW 去除了上下文各词的词序信息,使用上下文各词词向量的平均值,代替神经网络语言模型使用的上文各词词向量的拼接。 网络输入输出 目标: 最大化 $P (w_{i-(n-1)/2} | w_{i−(n−1)} , . . . , w_{i−1} )$输入: 整个词序列 $w_{i−(n−1)} , . . . , w_{i−1}$输出: 目标词的分布 网络结构示意图 输入层词$w_{i−(n−1)} , . . . , w_{i−1}$的词向量的组合 x = \frac{1}{n-1}\sum\limits _{w_j \in c}e(w_j)隐藏层h没有 输出层c表示上下文 P (w |c) = \frac {exp (e'(w)^Tx)}{\sum_{w' \in V}exp(e'(w')^Tx)}网络目标最大化 $\sum\limits_{(w,c)\in D}logP(w|c)$ Method 4: Skip-gram 模型与 CBOW 模型一样, Skip-gram 模型中也没有隐藏层。和 CBOW 模型不同的是,Skip-gram 模型每次从目标词 w 的上下文 c 中选择一个词,将其词向量作为模型的输入 x,也就是上下文的表示。 Skip-gram模型同样通过上下文预测目标词,对于整个语料的优化目标为最大化: \sum_{(w,c) \in D}\sum_{w_j \in c}log P (w|w_j ) 在 Skip-gram 的论文 中将模型描述成通过目标词预测上下文。由于模型需要遍历整个语料,任意一个窗口中的两个词 w_a , w_b 都需要计算 P (w_a |w_b ) + P (w_b |w_a ),因此这两种描述方式是等价的。 网络输入输出 目标: 最大化 $P (w_i | w_{i−(n−1)} , . . . , w_{i−1} )$输入: 整个词序列 $w_{i−(n−1)} , . . . , w_{i−1}$输出: 目标词的分布 网络结构示意图 输入层词$w_{i−(n−1)} , . . . , w_{i−1}$的词向量的组合 x = \frac{1}{n-1}\sum\limits _{w_j \in c}e(w_j)隐藏层h没有 输出层c表示上下文 P (w |c) = \frac {exp (e'(w)^Tx)}{\sum_{w' \in V}exp(e'(w')^Tx)}网络目标最大化 $\sum\limits _{(w,c) \in D}\sum\limits _{w_j \in c}log P (w|w_j )$ 采用手段负采样技术(negative sampling)构造负样本,构造出了一个优化目标,最大化正样本的似然,同时最小化负样本的似然。其中训练时,一个正样本可以对应多个负样本. 二次采样技术(subsampling)在大规模语料中,高频词通常就是停用词.一方面,这些高频词只能带来非常少量的语义信息.另一方面,训练高频词本身占据了大量的时间,但在迭代过程中,这些高频词的词向量变化并不大。如果词 w 在语料中的出现频率 $f (w) $大于阈值 t,则有 $P (w)$ 的概率在训练时跳过这个词。 P (w) = \frac{f (w) − t}{f (w)} - \sqrt{\frac{t}{f(w)}}模型总结 模型名称 上下文 隐含层 词序 NNLM n-gram(前n个词) 非线性变换 有词序 RNNLM 前面所有词 非线性变换 有词序 CBOW n-gram(前后n个词) 无,线性组合 无词序 NNLM n-gram(前或者后面1个词) 无 无词序 研究者进行一些对比试验,因需要引入其他知识,此处暂不列出.他们得出以下结论. 一、简单模型在小语料上整体表现更好,而复杂的模型需要更大的语料作支撑。二、对于实际的自然语言处理任务,各模型的差异不大。三、同领域的语料,语料越大效果越好。四、领域内的语料对相似领域任务的效果提升非常明显,但在领域不契合时甚至会有负面作用。五、包含的语义特征更为丰富的语料(而不只是数量),在评价语义特性的任务中,效果也更好。六、词向量的维度一般需要选择 50 维及以上,特别当衡量词向量的语言学特性时,词向量的维度越大,效果越好。 注:本文部分内容总结自来斯惟博士的毕业论文&lt;&lt;基于神经网络的词和文档语义向量表示方法研究&gt;&gt;.在此表示感谢,另附上来博士的博客地址: http://licstar.net/archives/tag/%E8%AF%8D%E5%90%91%E9%87%8F]]></content>
      <categories>
        <category>NLP</category>
        <category>summary</category>
      </categories>
      <tags>
        <tag>embedding</tag>
        <tag>neural_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习笔记（一）]]></title>
    <url>%2F2017%2F05%2F11%2Ftensorflow-study-1%2F</url>
    <content type="text"><![CDATA[TF的编程模型来源：http://blog.csdn.net/tinyzhao/article/details/52755647 计算图在TensorFlow中，算法都被表示成计算图（computational graphs）。计算图也叫数据流图,图中的节点表示操作，图中的边代表在不同操作之间的数据流动。在这样的数据流图中，有四个主要的元素概念： 操作(operations) 张量(tensors) 变量(variables) 会话(sessions) 操作(operations)数据流过节点的时候对数据进行的操作，操作包含很多种。 操作类型 例子 元素运算 Add,Mul 矩阵运算 MatMul,MatrixInverse 数值产生 Constant,Variable 神经网络单元 SoftMax,ReLU,Conv2D I/O Save,Restore 张量(tensors)图中的每个边代表数据从一个操作流到另一个操作。这些数据被表示为张量。一个张量可以看做是多维的数组或者高维的矩阵。张量本身并没有保存任何值，张量仅仅提供了访问数值的一个接口，可以看做是数值的一种引用。在TensorFlow实际使用中我们也可以发现，在run之前的张量并没有分配空间，此时的张量仅仅表示了一种数值的抽象，用来连接不同的节点，表示数据在不同操作之间的流动。TensorFlow中还提供了SparseTensor数据结构，用来表示稀疏张量。 变量(variables)可以改变数值和数据类型的节点。在实际处理时，一般把需要训练的值指定为变量。在使用变量的时候，需要指定变量的初始值，变量的大小和数据类型就是根据初始值来推断的。在构建计算图的时候，指定一个变量实际上需要增加三个节点： 实际的变量节点 一个产生初始值的操作，通常是一个常量节点 一个初始化操作，把初始值赋予到变量 如图所示，v代表的是实际的变量，i是产生初始值的节点，上面的assign节点将初始值赋予变量，assign操作以后，产生已经初始化的变量值v’。 tensorflow的基本运作方式12345678910111213#coding=utf-8import tensorflow as tf#定义占位符a = tf.placeholder(tf.int32,shape=())b = tf.placeholder(tf.int32,shape=())#构建运算图y = tf.multiply(a, b) #构造一个op节点#创建会话sess = tf.Session()#运行会话，输入数据，并计算节点，同时打印结果print(sess.run(y, feed_dict=&#123;a: 3, b: 3&#125;))# 任务完成, 关闭会话.sess.close() TF的运行流程来源：http://www.cnblogs.com/wuzhitj/p/6297734.html 概念描述Tensor在训练开始前，所有的数据都是抽象的概念1234import tensorflow as tf # 在下面所有代码中，都去掉了这一行，默认已经导入a = tf.zeros(shape=[1,2])print(a)#Tensor("zeros:0", shape=(1, 2), dtype=float32) 只有在训练过程开始后，才能获得a的实际值123sess = tf.InteractiveSession()print(sess.run(a))#[[ 0. 0.]] Variable故名思议，是变量的意思,与Tensor不同，Variable必须初始化以后才有具体的值1234567tensor = tf.zeros(shape=[1,2])variable = tf.Variable(tensor)sess = tf.InteractiveSession()# print(sess.run(variable)) # 会报错sess.run(tf.initialize_all_variables()) # 对variable进行初始化print(sess.run(variable))#[[ 0. 0.]] placeholder又叫占位符，同样是一个抽象的概念。用于表示输入输出数据的格式。告诉系统：这里有一个值/向量/矩阵，现在我没法给你具体数值，不过我正式运行的时候会补上的！例如上式中的x和y。因为没有具体数值，所以只要指定尺寸即可。12x = tf.placeholder(tf.float32,[1, 5],name='input')y = tf.placeholder(tf.float32,[None, 5],name='input') 上面有两种形式，第一种x，表示输入是一个[1,5]的横向量。而第二种形式，表示输入是一个[?,5]的矩阵。那么什么情况下会这么用呢?就是需要输入一批[1,5]的数据的时候。比如我有一批共10个数据，那我可以表示成[10,5]的矩阵。如果是一批5个，那就是[5,5]的矩阵。tensorflow会自动进行批处理 Sessionsession，也就是会话。session是抽象模型的实现者。只有实现了模型以后，才能够得到具体的值。]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
</search>