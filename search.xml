<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2017%2F05%2F19%2Ftest%2F</url>
    <content type="text"><![CDATA[R_{m \times n} = U_{m \times m} S_{m \times n} V_{n \times n}'\$R{m \times n} = U{m \times m} S{m \times n} V{n \times n}’$ $R{m \times n} = U{m \times m} S{m \times n} V{n \times n}’$]]></content>
  </entry>
  <entry>
    <title><![CDATA[词向量 word embedding]]></title>
    <url>%2F2017%2F05%2F19%2Fword-embedding%2F</url>
    <content type="text"><![CDATA[Objective 问题起源 文本是符号数据,两个词只要字面不同,就难以刻画它们之间的联系,即使是“麦克风”和“话筒”这样的同义词,从字面上也难以看出这两者意思相同(语义鸿沟现象)。（符号性这个角度类似于生物序列数据） 我们希望计算机可以从大规模无标注的文本数据中自动学习得到文本表示,这种表示需要包含对应语言单元(词或文档)的语义信息,同时可以直接通过这种表示度量文本之间的语义相似度。 形式化问题 1 输入：语句的集合/文章的集合 输出: 语言中词的一种表示 1954年，Harris 提出分布假说(distributional hypothesis),即“上下文相似的词,其语义也相似“。（只针对词） 在分布假说中,需要关注的对象有两个:词和上下文,其中最关键的是上下文的表示。 形式化问题 2 输入：词的上下文 输出: 词的一种表示 上下文只能使用传统的词袋子表示(后面解释),如果需要表示复杂的上下文,会遇到维数灾难问题。 神经网络模型可以使用组合方式(?)对上下文进行建模,只需线性复杂度即可对复杂的 n 元短语进行建模。神经网络模型生成的词表示通常被称为词向量(word embedding)。 &gt; 词向量：一个低维的实数向量表示。通过这种表示,可以直接对词之间的相似度进行刻画。 问题形式化 输入: 词的上下文 输出: 词的低维实数向量表示 方法: 神经网络 Related Knowledge 符号表示 \(w\) 表示一个词语. $w_1 , w_2 , …, w_m $表示一条由m个词语组成的语句. 语言模型 语言模型可以对一段文本的概率进行估计. 形式化的讲,统计语言模型的作用是为一个长度为 \(m\) 的字符串确定一个概率分布 \(P (w_1 , w_2 , ..., w_m )\) \[ P(w_1 , w_2 , ..., w_m ) = P (w_1 ) P (w_2 |w_1 ) \dots P (w_i | w_1 , w_2 , ..., w_{i−1} )\dots P (w_m | w_1 , w_2 , ..., w_{m−1} ) \] \(n\)元(n-gram)模型 如果文本的长度较长,公式右部$ P (w_i | w_1 , w_2 , , w_{i−1} ) \(的估算会非常困难。 因此,研究者们提出使用一个简化模型:\)n$ 元模型.对上述条件概率做了以下近似: \[ P (w_i | w_1 , w_2 , ..., w_i−1 ) ≈ P (w_i | w_{i−(n−1)} , . . . , w_i−1 ) \] 在 n 元模型中,传统的方法一般采用频率计数的比例来估算 \(n\) 元条件概率: \[ P (w_i | w_{i−(n−1)} , . . . , w_{i−1} ) = \frac{count(w_{i−(n−1)} , . . . , w_{i−1} , w_i ) }{count(w_{i−(n−1)} , . . . , w_{i−1} )} \] 其中,\(count(w_{i−(n−1)} , . . . , w_{i−1} )\) 表示文本序列 \(w_{i−(n−1)} , . . . , w_{i−1}\) 在语料中出现的次数。 Method Method 1: 神经网络语言模型(NNLM) RNNLM 利用前n个词的上文信息,预测下一个词.即,直接对$ P (w_i | w_1 , w_2 , …, w_{i−1} ) $进行建模. RNNLM是利用神经网络对条件概率建模. 网络输入输出 目标: 最大化 \(P (w_{i-(n-1)/2}| w_{i−(n−1)} , . . . , w_{i−1} )\) 输入: 整个词序列 \(w_{i−(n−1)} , . . . , w_{i−1}\) 输出: 目标词的分布 网络结构 示意图 输入层 词\(w_{i−(n−1)} , . . . , w_{i−1}\)的词向量的顺序拼接 \[x = [e(w_{i−(n−1)} ); . . . ; e(w_{i−2} ); e(w_{i−1} )]\] 隐藏层h \[h= tanh(b^{(1)} + Hx)\] \[y = b ^{(2)} + W x + U h\] 其中,其中,\(φ\) 为非线性激活函数。 根据公式,每一个隐藏层包含了当前词的信息以及上一个隐藏层的信息。通过这种迭代推进的方式,每个隐藏层实际上包含了此前所有上文的信息. 输出层 \[ P (w_i | w_{i−(n−1)} , . . . , w_{i−1} ) = \frac {exp (y(w_i))}{\sum_{k=1}^{|V|}exp(y(v_k))}\] 网络目标 最大化 \(\sum\limits _{w _{i−(n−1):i} ∈D}log P (w_i | w_{i−(n−1)} ,\dots, w_{i−1} )\) 即最大化语言模型. Method 2: 循环神经网络语言模型(RNNLM) RNNLM 利用所有的上文信息,预测下一个词.即,直接对$ P (w_i | w_1 , w_2 , …, w_{i−1} ) $进行建模. RNNLM是利用神经网络对条件概率建模. 网络输入输出 目标: 最大化 \(P (w_i | w_{i−(n−1)} , . . . , w_1 )\) 输入: 整个词序列 \(w_{i−(n−1)} , . . . , w_{i−1}\) 输出: 目标词的分布 网络结构 示意图 输入层 词\(w_{i−(n−1)} , . . . , w_{i−1}\)的词向量的顺序拼接 \[x = [e(w_{i−(n−1)} ); . . . ; e(w_{i−2} ); e(w_{i−1} )]\] 隐藏层h \[h(i) = φ(e(w_i ) + w_h(i − 1))\] \[y(i) = b + uh(i)\] 其中,其中,\(φ\) 为非线性激活函数。 根据公式,每一个隐藏层包含了当前词的信息以及上一个隐藏层的信息。通过这种迭代推进的方式,每个隐藏层实际上包含了此前所有上文的信息. 输出层 \[ P (w_i | w_{i−(n−1)} , . . . , w_1 ) = \frac {exp (y(w_i))}{\sum_{k=1}^{|V|}exp(y(v_k))}\] 网络目标 最大化语言模型\(\sum\limits _{w _{i−(n−1):i} ∈D}log P (w_i | w_{i−(n−1)} ,\dots, w_1 )\) Method 3: CBOW 模型 该模型一方面使用一段文本的中间词作为目标词;另一方面,又以 NNLM 作为蓝本,并在其基础上做了两个简化。 一、CBOW 没有隐藏层,去掉隐藏层之后,模型从神经网络结构直接转化为 log 线性结构,与 Logistic 回归一致。log 线性结构比三层神经网络结构少了一个矩阵运算,大幅度地提升了模型的训练速度。 二、CBOW 去除了上下文各词的词序信息,使用上下文各词词向量的平均值,代替神经网络语言模型使用的上文各词词向量的拼接。 网络输入输出 目标: 最大化 \(P (w_{i-(n-1)/2} | w_{i−(n−1)} , . . . , w_{i−1} )\) 输入: 整个词序列 \(w_{i−(n−1)} , . . . , w_{i−1}\) 输出: 目标词的分布 网络结构 示意图 ###### 输入层 词\(w_{i−(n−1)} , . . . , w_{i−1}\)的词向量的组合 \[x = \frac{1}{n-1}\sum\limits _{w_j \in c}e(w_j)\] 隐藏层h 没有 输出层 c表示上下文 \[ P (w |c) = \frac {exp (e&#39;(w)^Tx)}{\sum_{w&#39; \in V}exp(e&#39;(w&#39;)^Tx)} \] 网络目标 最大化 \(\sum\limits_{(w,c)\in D}logP(w|c)\) Method 4: Skip-gram 模型 与 CBOW 模型一样, Skip-gram 模型中也没有隐藏层。和 CBOW 模型不同的是,Skip-gram 模型每次从目标词 w 的上下文 c 中选择一个词,将其词向量作为模型的输入 x,也就是上下文的表示。 Skip-gram模型同样通过上下文预测目标词,对于整个语料的优化目标为最大化: \[\sum_{(w,c) \in D}\sum_{w_j \in c}log P (w|w_j )\] &gt; 在 Skip-gram 的论文 中将模型描述成通过目标词预测上下文。由于模型需要遍历整个语料,任意一个窗口中的两个词 w_a , w_b 都需要计算 P (w_a |w_b ) + P (w_b |w_a ),因此这两种描述方式是等价的。 网络输入输出 目标: 最大化 \(P (w_i | w_{i−(n−1)} , . . . , w_{i−1} )\) 输入: 整个词序列 \(w_{i−(n−1)} , . . . , w_{i−1}\) 输出: 目标词的分布 网络结构 示意图 输入层 词\(w_{i−(n−1)} , . . . , w_{i−1}\)的词向量的组合 \[x = \frac{1}{n-1}\sum\limits _{w_j \in c}e(w_j)\] 隐藏层h 没有 输出层 c表示上下文 \[ P (w |c) = \frac {exp (e&#39;(w)^Tx)}{\sum_{w&#39; \in V}exp(e&#39;(w&#39;)^Tx)} \] 网络目标 最大化 \(\sum\limits _{(w,c) \in D}\sum\limits _{w_j \in c}log P (w|w_j )\) 采用手段 负采样技术(negative sampling) 构造负样本,构造出了一个优化目标,最大化正样本的似然,同时最小化负样本的似然。其中训练时,一个正样本可以对应多个负样本. 二次采样技术(subsampling) 在大规模语料中,高频词通常就是停用词.一方面,这些高频词只能带来非常少量的语义信息.另一方面,训练高频词本身占据了大量的时间,但在迭代过程中,这些高频词的词向量变化并不大。 如果词 w 在语料中的出现频率 $f (w) $大于阈值 t,则有 \(P (w)\) 的概率在训练时跳过这个词。 \[ P (w) = \frac{f (w) − t}{f (w)} - \sqrt{\frac{t}{f(w)}} \] 模型总结 | 模型名称 | 上下文 |隐含层|词序| |:—:—|—:—|—:—|—:—| | NNLM | n-gram(前n个词) | 非线性变换|有词序| | RNNLM | 前面所有词 | 非线性变换|有词序| | CBOW | n-gram(前后n个词) | 无,线性组合|无词序| | NNLM | n-gram(前或者后面1个词) | 无|无词序| 研究者进行一些对比试验,因需要引入其他知识,此处暂不列出.他们得出以下结论. 一、简单模型在小语料上整体表现更好,而复杂的模型需要更大的语料作支撑。 二、对于实际的自然语言处理任务,各模型的差异不大。 三、同领域的语料,语料越大效果越好。 四、领域内的语料对相似领域任务的效果提升非常明显,但在领域不契合时甚至会有负面作用。 五、包含的语义特征更为丰富的语料(而不只是数量),在评价语义特性的任务中,效果也更好。 六、词向量的维度一般需要选择 50 维及以上,特别当衡量词向量的语言学特性时,词向量的维度越大,效果越好。 注:本文内容总结自来斯惟博士的毕业论文 在此表示感谢,另附上来博士的博客地址: http://licstar.net/archives/tag/%E8%AF%8D%E5%90%91%E9%87%8F]]></content>
      <categories>
        <category>NLP</category>
        <category>summary</category>
      </categories>
      <tags>
        <tag>embedding</tag>
        <tag>neural_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习笔记（一）]]></title>
    <url>%2F2017%2F05%2F11%2Ftensorflow-study-1%2F</url>
    <content type="text"><![CDATA[TF的编程模型来源：http://blog.csdn.net/tinyzhao/article/details/52755647 计算图在TensorFlow中，算法都被表示成计算图（computational graphs）。计算图也叫数据流图,图中的节点表示操作，图中的边代表在不同操作之间的数据流动。在这样的数据流图中，有四个主要的元素概念： 操作(operations) 张量(tensors) 变量(variables) 会话(sessions) 操作(operations)数据流过节点的时候对数据进行的操作，操作包含很多种。 操作类型 例子 元素运算 Add,Mul 矩阵运算 MatMul,MatrixInverse 数值产生 Constant,Variable 神经网络单元 SoftMax,ReLU,Conv2D I/O Save,Restore 张量(tensors)图中的每个边代表数据从一个操作流到另一个操作。这些数据被表示为张量。一个张量可以看做是多维的数组或者高维的矩阵。张量本身并没有保存任何值，张量仅仅提供了访问数值的一个接口，可以看做是数值的一种引用。在TensorFlow实际使用中我们也可以发现，在run之前的张量并没有分配空间，此时的张量仅仅表示了一种数值的抽象，用来连接不同的节点，表示数据在不同操作之间的流动。TensorFlow中还提供了SparseTensor数据结构，用来表示稀疏张量。 变量(variables)可以改变数值和数据类型的节点。在实际处理时，一般把需要训练的值指定为变量。在使用变量的时候，需要指定变量的初始值，变量的大小和数据类型就是根据初始值来推断的。在构建计算图的时候，指定一个变量实际上需要增加三个节点： 实际的变量节点 一个产生初始值的操作，通常是一个常量节点 一个初始化操作，把初始值赋予到变量 如图所示，v代表的是实际的变量，i是产生初始值的节点，上面的assign节点将初始值赋予变量，assign操作以后，产生已经初始化的变量值v’。 tensorflow的基本运作方式12345678910111213#coding=utf-8import tensorflow as tf#定义占位符a = tf.placeholder(tf.int32,shape=())b = tf.placeholder(tf.int32,shape=())#构建运算图y = tf.multiply(a, b) #构造一个op节点#创建会话sess = tf.Session()#运行会话，输入数据，并计算节点，同时打印结果print(sess.run(y, feed_dict=&#123;a: 3, b: 3&#125;))# 任务完成, 关闭会话.sess.close() TF的运行流程来源：http://www.cnblogs.com/wuzhitj/p/6297734.html 概念描述Tensor在训练开始前，所有的数据都是抽象的概念1234import tensorflow as tf # 在下面所有代码中，都去掉了这一行，默认已经导入a = tf.zeros(shape=[1,2])print(a)#Tensor("zeros:0", shape=(1, 2), dtype=float32) 只有在训练过程开始后，才能获得a的实际值123sess = tf.InteractiveSession()print(sess.run(a))#[[ 0. 0.]] Variable故名思议，是变量的意思,与Tensor不同，Variable必须初始化以后才有具体的值1234567tensor = tf.zeros(shape=[1,2])variable = tf.Variable(tensor)sess = tf.InteractiveSession()# print(sess.run(variable)) # 会报错sess.run(tf.initialize_all_variables()) # 对variable进行初始化print(sess.run(variable))#[[ 0. 0.]] placeholder又叫占位符，同样是一个抽象的概念。用于表示输入输出数据的格式。告诉系统：这里有一个值/向量/矩阵，现在我没法给你具体数值，不过我正式运行的时候会补上的！例如上式中的x和y。因为没有具体数值，所以只要指定尺寸即可。12x = tf.placeholder(tf.float32,[1, 5],name='input')y = tf.placeholder(tf.float32,[None, 5],name='input') 上面有两种形式，第一种x，表示输入是一个[1,5]的横向量。而第二种形式，表示输入是一个[?,5]的矩阵。那么什么情况下会这么用呢?就是需要输入一批[1,5]的数据的时候。比如我有一批共10个数据，那我可以表示成[10,5]的矩阵。如果是一批5个，那就是[5,5]的矩阵。tensorflow会自动进行批处理 Sessionsession，也就是会话。session是抽象模型的实现者。只有实现了模型以后，才能够得到具体的值。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo usage]]></title>
    <url>%2F2017%2F05%2F10%2Fstart%2F</url>
    <content type="text"><![CDATA[Create a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>