<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[evaluation-measure]]></title>
    <url>%2F2017%2F09%2F07%2Fevaluation-measure%2F</url>
    <content type="text"><![CDATA[分类问题precision(精确率)P = \frac{TP}{TP+FP} \tag{1}recall(召回率)R = \frac{TP}{TP+FN} \tag{2}$F_1$精确率和召回率的调和均值， \begin{align*} \frac{2}{F_1} & = \frac{1}{P} + \frac{1}{R}\\ F_1 & = \frac{2TP}{2TP + FP + FN} \tag{3} \end{align*}精确率和准确率都高的情况下，$F_1$值也会高。 laterhttp://charleshm.github.io/2016/03/Model-Performance/]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>性能评估指标</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F09%2F01%2FAlignment_Quality_Evaluation%2F</url>
    <content type="text"><![CDATA[sequences alignment toolsBLASTsequence-sequence alignment PSI-BLASTprofile-sequence alignment HMMERHMM-sequence alignment PROF_SIM and COMPASSprofile-profile alignment HHSearchHHSearch2Alignment Quality Evaluationplain MaxSub scoreS_{Dev}=N_{correct}/min(L_q,L_p)Modeler’s scoreS_{Mod}=N_{correct}/L_{ali}Balanced ScoreS_{balanced}=(S_{Dev}+S_{Mod})]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F08%2F27%2Ftf_train%2F</url>
    <content type="text"><![CDATA[MonitoredSessionhttps://www.tensorflow.org/api_docs/python/tf/train/MonitoredSessionhook Experimenthttps://www.tensorflow.org/api_docs/python/tf/contrib/learn/ExperimentExperiment is a class containing all information needed to train a model.by passing an Estimator and inputs for training and evaluation, an Experiment instance knows how to invoke training and eval loops in a sensible fashion for distributed training. methodConstructor1234567891011121314151617__init__( estimator, train_input_fn, eval_input_fn, eval_metrics=None, train_steps=None, eval_steps=100, train_monitors=None, eval_hooks=None, local_eval_frequency=None, eval_delay_secs=120, continuous_eval_throttle_secs=60, min_eval_frequency=None, delay_workers_by_global_step=False, export_strategies=None, train_steps_per_iteration=None) Creates an Experiment instance.None of the functions passed to this constructor are executed at construction time.They are stored and used when a method is executed which requires it. estimator Object implementing Estimator interface, train_input_fn function, returns features and labels for training. eval_input_fn function, returns features and labels for evaluation. If eval_steps is None, this should be configured only to produce for a finite number of batches (generally, 1 epoch over the evaluation data). eval_metrics dict of string, metric function. If None, default set is used. This should be None if the estimator is ${tf.estimator.Estimator}$. If metrics are provided they will be appended to the default set. train_steps Perform this many steps of training. None, the default, means train forever. eval_steps evaluate runs until input is exhausted (or another exception is raised), or for eval_steps steps, if specified. train_monitors A list of monitors to pass to the Estimator’s fit function. eval_hooks A list of SessionRunHook hooks to pass to the Estimator’s evaluate function. eval_delay_secs Start evaluating after waiting for this many seconds. continuous_eval_throttle_secs when the last evaluation was started at least this many seconds ago for continuous_eval(), re-evaluate. min_eval_frequency (applies only to train_and_evaluate). the minimum number of steps between evaluations. Of course, evaluation does not occur if no new snapshot is available, hence, this is the minimum. If 0, the evaluation will only happen after training. If None, defaults to 1, unless model_dir is on GCS, in which case the default is 1000. delay_workers_by_global_step if True delays training workers based on global step instead of time. export_strategies Iterable of ExportStrategys, or a single one, or None. train_steps_per_iteration (applies only to continuous_train_and_eval). Perform this many (integer) number of train steps for each training-evaluation iteration. With a small value, the model will be evaluated more frequently with more checkpoints saved. If None, will use a default value (which is smaller than train_steps if provided). supervisorbasic use process Create a Supervisor object, parameter: logdir the path to a directory where to save checkpoints and summaries. Ask the supervisor for a session with tf.train.Supervisor.managed_session Use the session to execute a train op, checking at each step if the supervisor requests that the training stops. started servicesThe managed_session() call starts a few services, which run in their own threads and use the managed session to run ops in your graph.If your graph contains an integer variable named global_step, the services use its value to measure the number of training steps executed. Checkpointing service: Saves a copy of the graph variables in the logdir. The checkpoint filename uses the value of the global_step variable if one was added to your graph. Runs every 10 minutes by default. Summary service: Runs all the summary ops and appends their output to an events file in the logdir. Runs every 2 minutes by default. Step counter: Counts how many steps have been executed, by looking at changes in the global_step variable. Appends a summary to the events file reporting the number of global steps per second. The summary tag is “global_step/sec”. This also runs every 2 minutes by default. Queue Runners: If any tf.train.QueueRunner were added to the graph, the supervisor launches them in their own threads. Checking for StopThe check for stop in the main training loop is important and necessary. Exceptions raised in the service threads are reported to the supervisor which then sets its should_stop() condition to true. Other service threads notice that condition and terminate properly. The main training loop, within the managed_session() block, must also check for the stop condition and terminate. Notice:managed_session() takes care of catching exceptions raised from the training loop to report them to the supervisor.The main loop does not need to do anything special about exceptions. It only needs to check for the stop condition. RecoveryIf the training program shuts down or crashes, its most recent checkpoint and event files are left in the logdir.When you restart the program, managed_session() restores the graph from the most recent checkpoint and resumes training where it stopped.A new events file is created. If you start TensorBoard and point it to the logdir, it will know how to merge the contents of the two events files and will show the training resuming at the last global step from the checkpoint. Larger Model ScenarioLarger models may run out memory when the summary service runs: The summary ops are run in parallel with the main loop running the train op. This can cause memory usage to peak to up to two times the normal use. For a larger model you can tell the supervisor to not run the summary service and instead run it yourself in your main training loop: pass summary_op=None when constructing the supervisor. 123456789101112131415...create graph...my_train_op = ...my_summary_op = tf.summary.merge_all()sv = tf.train.Supervisor(logdir="/my/training/directory", summary_op=None) # Do not run the summary servicewith sv.managed_session() as sess: for step in range(100000): if sv.should_stop(): break if step % 100 == 0: _, summ = sess.run([my_train_op, my_summary_op]) sv.summary_computed(sess, summ) else: sess.run(my_train_op) Pre-trained Model ScenarioThe managed_session() call takes care of initializing the model in the session If model is available, it is restored from a checkpoint. otherwise, initialized from scratch. One common scenario is to initialize the model by loading a “pre-trained” checkpoint that was saved while training a usually slightly different model using a different dataset. init function is called only if the model needs to be initialized from scratch not when the model can be recovered from a checkpoint from the logdir. To load the pre-trained model, the init function needs a tf.train.Saver object This saver must only restore the pre-trained variables This is usually a good idea because the new model may contain variables that are not present in the pre-trained checkpoint If you were using the default saver, you could get an error trying to restore all the variables of the new model from the pre-trained checkpoint. The process is below:123456789101112131415161718...create graph...# Create a saver that restores only the pre-trained variables.pre_train_saver = tf.train.Saver([pre_train_var1, pre_train_var2])# Define an init function that loads the pretrained checkpoint.def load_pretrain(sess): pre_train_saver.restore(sess, "&lt;path to pre-trained-checkpoint&gt;")# Pass the init function to the supervisor.## The init function is called _after_ the variables have been initialized# by running the init_op.sv = tf.train.Supervisor(logdir="/my/training/directory", init_fn=load_pretrain)with sv.managed_session() as sess: # Here sess was either initialized from the pre-trained-checkpoint or # recovered from a checkpoint saved in a previous run of this code. ... Running Your Own ServicesFor example to fetch different sets of summaries on a different schedule than the usual summary service.Use the tf.train.Supervisor.loopmethod It repeatedly calls a function of your choice on a timer until the supervisor stop condition becomes true It plays nicely with the other services. 1234567891011def my_additional_summaries(sv, sess): ...fetch and write summaries, see below...... sv = tf.train.Supervisor(logdir="/my/training/directory") with sv.managed_session() as sess: # Call my_additional_summaries() every 1200s, or 20mn, # passing (sv, sess) as arguments. sv.loop(1200, my_additional_summaries, args=(sv, sess)) ...main training loop... Writing SummariesThe supervisor always creates an events file in its logdir, as well as a tf.summary.FileWriter to append events and summaries to that file.If you want to write your own summaries it is a good idea to append them to that same events file TensorBoard likes it better when only one events file in a directory is being actively appended to. Method: tf.train.Supervisor.summary_computed123def my_additional_summaries(sv, sess): summaries = sess.run(my_additional_summary_op) sv.summary_computed(sess, summaries) For more advanced usages:tf.train.Supervisor.summary_writerhttps://www.tensorflow.org/api_docs/python/tf/train/Supervisor#summary_writer Supervisor ReferenceCheckpointing: Where and When.checkpointing service can be configured by the following keyword arguments to the Supervisor() constructor: logdir: path where the checkpointing service creates checkpoints. Passing None disables the checkpointing and the summary services. checkpoint_basename Name of the checkpoint files to create, defaults to “model.ckpt”. If the model contains a scalar integer variable named global_step, the value of that variable is appended to the checkpoint filename. save_model_secs Number of seconds between each checkpoint. Defaults to 600, or 10 minutes. saver A tf.train.Saver object to use for checkpointing. Default creates one for you by calling tf.train.Saver(), which add ops to save and restore all variables in your model. customer Saver need create customized Saver. Summaries: Where and Whenthe summary service can be configured by the following keyword arguments to the Supervisor() constructor: logdir Path to a directory where the summary service creates event files. Passing None disables the summary service as well as the checkpointing services. save_summaries_secs Number of seconds between each run of the summary service Defaults to 120, or 2 minutes. Pass 0 to disable the summary service. summary_op Op to use to fetch the summaries. Default use the first op in the tf.GraphKeys.SUMMARY_OP graph collection. If the collection is empty the supervisor creates an op that aggregates all summaries in the graph using tf.summary.merge_all() Passing None disables the summary service. global_step Tensor to use to count the global step. Default uses the first tensor in the tf.GraphKeys.GLOBAL_STEP graph collection. If the collection is empty, the supervisor looks for a scalar integer variable named global_step in the graph. If found, the global step tensor is used to measure the number of training steps executed Note that your training op is responsible for incrementing the global step value. Model Initialization and RecoveryinitializationThe managed_session() call takes care of initializing or recovering a session.It returns a session with a fully initialized model, ready to run ops.If a checkpoint exists in the logdir when managed_session() is called, the model is initialized by loading that checkpointotherwise it is initialized by calling an init op and optionally an init function.When no checkpoint is available, model initialization is controlled by the following keyword arguments to the Supervisor() constructor: init_op Op to run to initialize the model. Default uses the first op in the tf.GraphKeys.INIT_OP collection. If the collection is empty, the supervisor adds an op to initialize all the variables in the graph by calling tf.global_variables_initializer(). Pass None to not use an init op. init_fn Python function to call to initialize the model. If specified, called as init_fn(sess) where sess is the managed session. If an init op is also used, the init function is called after the init op. local_init_op An additional op to initialize parts of the graph that are not saved in checkpoints such as tables and local variables. The local init op is run before the init op and the init function. If not specified, the supervisor uses the first op in the tf.GraphKeys.LOCAL_INIT_OP collection. If the collection is empty the supervisor adds an op to initialize all the tables and local variables in the graph by calling tf.tables_initializer() and tf.local_variables_initializer(). Pass None to not use a local init op. ready_op Op to check if the model is initialized. After running the local init op, the init op, and the init function, the supervisor verifies that the model is fully initialized by running the ready op. This is an op that returns an empty string if the model is initialized, or a description of what parts of the model are not initialized if not. This is an op that returns an empty string if the model is initialized, or a description of what parts of the model are not initialized if not. If not specified, the supervisor uses the first op in the tf.GraphKeys.READY_OP collection. f the collection is empty the supervisor creates a ready op that verifies that all variables are initialized by calling tf.report_uninitialized_variables(). Pass None to disable the ready op. In that case the model is not checked after initialization. RecoveryCheckpoint recovery is controlled by the following keyword arguments to the Supervisor() constructor: logdir Path to a directory in which to look for checkpoints. The checkpoint service saves a metadata file, named “checkpoint”, in the checkpoint directory that indicates the path to the most recent checkpoint. This file is in text format. When in a pinch, you can edit it manually to recover from a different checkpoint than the most recent one. ready_op: (see above). The ready op is run before and after loading the checkpoint. The first run checks if the model needs to be initialized the second run verifies that the model is fully initialized. local_init_op: (see above). The local init op is run before running the ready op the first time, to initialize local variables and tables. saver: (see above). Saver object used to load the checkpoint.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F08%2F23%2Ftf_distributed%2F</url>
    <content type="text"><![CDATA[Easy demo12345678# Start a TensorFlow server as a single-process "cluster".$ python&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; c = tf.constant("Hello, distributed TensorFlow!")&gt;&gt;&gt; server = tf.train.Server.create_local_server()&gt;&gt;&gt; sess = tf.Session(server.target) # Create a session on the server.&gt;&gt;&gt; sess.run(c)'Hello, distributed TensorFlow!' The tf.train.Server.create_local_server method creates a single-process cluster, with an in-process server. Create a clusterclustera set of “tasks” that participate in the distributed execution of a TensorFlow graph. taskEach task is associated with a TensorFlow “server”. which contains a “master” that can be used to create sessions, and a “worker” that executes operations in the graph. jobsA cluster can also be divided into one or more “jobs”, where each job contains one or more tasks. create clusterstart one TensorFlow server per task in the cluster.Each task typically runs on a different machinebut you can run multiple tasks on the same machineIn each task, do the following: Create a tf.train.ClusterSpec that describes all of the tasks in the cluster. This should be the same for each task. Create a tf.train.Server passing the tf.train.ClusterSpec to the constructor, identifying the local task with a job name and task index. Create a tf.train.ClusterSpec to describe the clusterThe cluster specification dictionary maps job names to lists of network addresses.Pass this dictionary to the tf.train.ClusterSpec constructor. 1tf.train.ClusterSpec(&#123;"local": ["localhost:2222", "localhost:2223"]&#125;) /job:local/task:0/job:local/task:1 12345678910tf.train.ClusterSpec(&#123; "worker": [ "worker0.example.com:2222", "worker1.example.com:2222", "worker2.example.com:2222" ], "ps": [ "ps0.example.com:2222", "ps1.example.com:2222" ]&#125;) /job:worker/task:0/job:worker/task:1/job:worker/task:2/job:ps/task:0/job:ps/task:1 Create a tf.train.Server instance in each taskA tf.train.Server object contains a set of local devices, a set of connections to other tasks in its tf.train.ClusterSpec. a tf.Session that can use these to perform a distributed computation. Each server is a member of a specific named job and has a task index within that job.A server can communicate with any other server in the cluster. For example, to launch a cluster with two servers running on localhost:2222 and localhost:2223, run the following snippets in two different processes on the local machine:1234567# In task 0:cluster = tf.train.ClusterSpec(&#123;"local": ["localhost:2222", "localhost:2223"]&#125;)server = tf.train.Server(cluster, job_name="local", task_index=0)# In task 1:cluster = tf.train.ClusterSpec(&#123;"local": ["localhost:2222", "localhost:2223"]&#125;)server = tf.train.Server(cluster, job_name="local", task_index=1) Specifying distributed devices in your modelTo place operations on a particular process, you can use the same tf.device function that is used to specify whether ops run on the CPU or GPU. For example:123456789101112131415161718with tf.device("/job:ps/task:0"): weights_1 = tf.Variable(...) biases_1 = tf.Variable(...)with tf.device("/job:ps/task:1"): weights_2 = tf.Variable(...) biases_2 = tf.Variable(...)with tf.device("/job:worker/task:7"): input, labels = ... layer_1 = tf.nn.relu(tf.matmul(input, weights_1) + biases_1) logits = tf.nn.relu(tf.matmul(layer_1, weights_2) + biases_2) # ... train_op = ...with tf.Session("grpc://worker7.example.com:2222") as sess: for _ in range(10000): sess.run(train_op) In the above example, the variables are created on two tasks in the ps job, and the compute-intensive part of the model is created in the worker job. TensorFlow will insert the appropriate data transfers between the jobs (from ps to worker for the forward pass, and from worker to ps for applying gradients). Replicated trainingA common training configuration, called “data parallelism,” involves multiple tasks in a worker job training the same model on different mini-batches of data, updating shared parameters hosted in one or more tasks in a ps job. There are many ways to specify this structure in TensorFlow, and we are building libraries that will simplify the work of specifying a replicated model.Possible approaches include: In-graph replication. In this approach, the client builds a single tf.Graph that contains one set of parameters (in tf.Variable nodes pinned to /job:ps); and multiple copies of the compute-intensive part of the model, each pinned to a different task in /job:worker. Between-graph replication.? In this approach, there is a separate client for each /job:worker task, typically in the same process as the worker task. Each client builds a similar graph containing the parameters (pinned to /job:ps as before using tf.train.replica_device_setter to map them deterministically to the same tasks) and a single copy of the compute-intensive part of the model, pinned to the local task in /job:worker. Asynchronous training.? In this approach, each replica of the graph has an independent training loop that executes without coordination. It is compatible with both forms of replication above. Synchronous training. n this approach, all of the replicas read the same values for the current parameters, compute gradients in parallel, and then apply them together. It is compatible with in-graph replication (e.g. using gradient averaging as in the CIFAR-10 multi-GPU trainer), and between-graph replication (e.g. using the tf.train.SyncReplicasOptimizer). Putting it all together: example trainer programThe following code shows the skeleton of a distributed trainer program1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import argparseimport sysimport tensorflow as tfFLAGS = Nonedef main(_): ps_hosts = FLAGS.ps_hosts.split(",") worker_hosts = FLAGS.worker_hosts.split(",") # Create a cluster from the parameter server and worker hosts. cluster = tf.train.ClusterSpec(&#123;"ps": ps_hosts, "worker": worker_hosts&#125;) # Create and start a server for the local task. server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index) if FLAGS.job_name == "ps": server.join() elif FLAGS.job_name == "worker": # Assigns ops to the local worker by default. with tf.device(tf.train.replica_device_setter( worker_device="/job:worker/task:%d" % FLAGS.task_index, cluster=cluster)): # Build model... loss = ... global_step = tf.contrib.framework.get_or_create_global_step() train_op = tf.train.AdagradOptimizer(0.01).minimize( loss, global_step=global_step) # The StopAtStepHook handles stopping after running given steps. hooks=[tf.train.StopAtStepHook(last_step=1000000)] # The MonitoredTrainingSession takes care of session initialization, # restoring from a checkpoint, saving to a checkpoint, and closing when done # or an error occurs. with tf.train.MonitoredTrainingSession(master=server.target, is_chief=(FLAGS.task_index == 0), checkpoint_dir="/tmp/train_logs", hooks=hooks) as mon_sess: while not mon_sess.should_stop(): # Run a training step asynchronously. # See `tf.train.SyncReplicasOptimizer` for additional details on how to # perform *synchronous* training. # mon_sess.run handles AbortedError in case of preempted PS. mon_sess.run(train_op)if __name__ == "__main__": parser = argparse.ArgumentParser() parser.register("type", "bool", lambda v: v.lower() == "true") # Flags for defining the tf.train.ClusterSpec parser.add_argument( "--ps_hosts", type=str, default="", help="Comma-separated list of hostname:port pairs" ) parser.add_argument( "--worker_hosts", type=str, default="", help="Comma-separated list of hostname:port pairs" ) parser.add_argument( "--job_name", type=str, default="", help="One of 'ps', 'worker'" ) # Flags for defining the tf.train.Server parser.add_argument( "--task_index", type=int, default=0, help="Index of task within the job" ) FLAGS, unparsed = parser.parse_known_args() tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) GlossaryClientA client is typically a program that builds a TensorFlow graph and constructs a tensorflow::Session to interact with a cluster.A single client process can directly interact with multiple TensorFlow servers (see “Replicated training” above), and a single server can serve multiple clients. ClusterA TensorFlow cluster comprises a one or more “jobs”, each divided into lists of one or more “tasks”.A cluster is typically dedicated to a particular high-level objective, such as training a neural network, using many machines in parallel.A cluster is defined by a tf.train.ClusterSpec object. JobA job comprises a list of “tasks”, which typically serve a common purpose.For example, a job named ps (for “parameter server”) typically hosts nodes that store and update variableswhile a job named worker typically hosts stateless nodes that perform compute-intensive tasks.The tasks in a job typically run on different machines.The set of job roles is flexible: for example, a worker may maintain some state. Master serviceAn RPC service that provides remote access to a set of distributed devices, and acts as a session target.The master service implements the tensorflow::Session interface, and is responsible for coordinating work across one or more “worker services”.All TensorFlow servers implement the master service. TaskA task corresponds to a specific TensorFlow server, and typically corresponds to a single process. A task belongs to a particular “job” and is identified by its index within that job’s list of tasks. TensorFlow serverA process running a tf.train.Server instance, which is a member of a cluster, and exports a “master service” and “worker service”. Worker serviceAn RPC service that executes parts of a TensorFlow graph using its local devices. A worker service implements worker_service.proto. All TensorFlow servers implement the worker service.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F08%2F19%2Ftf_supervisor%2F</url>
    <content type="text"><![CDATA[supervisorhttps://www.tensorflow.org/programmers_guide/supervisor]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F08%2F19%2Ftf_experiment%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F08%2F19%2Ftf_estimator%2F</url>
    <content type="text"><![CDATA[tf.estimator 学习基本结构两个基本参数: model_fn and params1nn = tf.estimator.Estimator(model_fn=model_fn, params=model_params) model_fn: A function object that contains all the aforementioned logic to support training, evaluation, and prediction. You are responsible for implementing that functionality. params: An optional dict of hyperparameters (e.g., learning rate, dropout) that will be passed into the model_fn. Notice:The Estimator also accepts the general configuration arguments model_dir and config. 构建model_fn基本骨架Input: 接受参数 features A dict containing the features passed to the model via input_fn. labels A Tensor containing the labels passed to the model via input_fn Will be empty for predict() calls, as these are the values the model will infer. mode tf.estimator.ModeKeys.TRAIN The model_fn was invoked in training mode, namely via a train() call tf.estimator.ModeKeys.EVAL The model_fn was invoked in evaluation mode, namely via an evaluate() call. tf.estimator.ModeKeys.PREDICT The model_fn was invoked in predict mode, namely via a predict() call. params containing a dict of hyperparameters used for training body: 逻辑过程 Configure the model a neural network Define the loss function to calculate how closely the model’s predictions match the target values Define the training operation to specify the optimizer algorithm to minimize the loss values calculated by the loss function. Generate predictions Return predictions/loss/train_op/eval_metric_ops in EstimatorSpec object 1return EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops) output: 返回必须返回一个tf.estimator.EstimatorSpec对象1return EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops) mode (required in all mode) 直接将model_fn的mode传递下去 predictions (required in PREDICT mode) A dict that maps key names of your choice to Tensors containing the predictions from the model. In PREDICT mode, the dict that you return in EstimatorSpec will then be returned by predict() you can construct it in the format in which you’d like to consume it loss ((required in EVAL and TRAIN mode)) A Tensor containing a scalar loss value the output of the model’s loss function calculated over all the input examples is used in TRAIN mode for error handling and logging is automatically included as a metric in EVAL mode. train_op (required only in TRAIN mode). An Op that runs one step of training eval_metric_ops (optional) A dict of name/value pairs specifying the metrics that will be calculated when the model runs in EVAL mode. The name is a label of your choice for the metric the value is the result of your metric calculation. The tf.metrics module provides predefined functions for a variety of common metrics. 1eval_metric_ops = &#123; "accuracy": tf.metrics.accuracy(labels, predictions) &#125; 相关APIoptimizers]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F08%2F19%2FUntitled%2F</url>
    <content type="text"><![CDATA[tf.estimator 学习基本结构两个基本参数: model_fn and params1nn = tf.estimator.Estimator(model_fn=model_fn, params=model_params) model_fn: A function object that contains all the aforementioned logic to support training, evaluation, and prediction. You are responsible for implementing that functionality. params: An optional dict of hyperparameters (e.g., learning rate, dropout) that will be passed into the model_fn. Notice:The Estimator also accepts the general configuration arguments model_dir and config. 构建model_fn基本骨架Input: 接受参数 features A dict containing the features passed to the model via input_fn. labels A Tensor containing the labels passed to the model via input_fn Will be empty for predict() calls, as these are the values the model will infer. mode tf.estimator.ModeKeys.TRAIN The model_fn was invoked in training mode, namely via a train() call tf.estimator.ModeKeys.EVAL The model_fn was invoked in evaluation mode, namely via an evaluate() call. tf.estimator.ModeKeys.PREDICT The model_fn was invoked in predict mode, namely via a predict() call. params containing a dict of hyperparameters used for training body: 逻辑过程 Configure the model a neural network Define the loss function to calculate how closely the model’s predictions match the target values Define the training operation to specify the optimizer algorithm to minimize the loss values calculated by the loss function. Generate predictions Return predictions/loss/train_op/eval_metric_ops in EstimatorSpec object 1return EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops) output: 返回必须返回一个tf.estimator.EstimatorSpec对象1return EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops) mode (required in all mode) 直接将model_fn的mode传递下去 predictions (required in PREDICT mode) A dict that maps key names of your choice to Tensors containing the predictions from the model. In PREDICT mode, the dict that you return in EstimatorSpec will then be returned by predict() you can construct it in the format in which you’d like to consume it loss ((required in EVAL and TRAIN mode)) A Tensor containing a scalar loss value the output of the model’s loss function calculated over all the input examples is used in TRAIN mode for error handling and logging is automatically included as a metric in EVAL mode. train_op (required only in TRAIN mode). An Op that runs one step of training eval_metric_ops (optional) A dict of name/value pairs specifying the metrics that will be calculated when the model runs in EVAL mode. The name is a label of your choice for the metric the value is the result of your metric calculation. The tf.metrics module provides predefined functions for a variety of common metrics. 1eval_metric_ops = &#123; "accuracy": tf.metrics.accuracy(labels, predictions) &#125; 相关APIoptimizers]]></content>
  </entry>
  <entry>
    <title><![CDATA[tf.Variable and tf.Tensor]]></title>
    <url>%2F2017%2F08%2F18%2Ftf-variable%2F</url>
    <content type="text"><![CDATA[tf.VariableCreate variabletf.get_variable easy to define models which reuse layers 12my_int_variable = tf.get_variable("my_int_variable", [1, 2, 3], dtype=tf.int32, initializer=tf.zeros_initializer) 12other_variable = tf.get_variable("other_variable", dtype=tf.int32, initializer=tf.constant([23, 42])) Variable collectionscollections: name lists of tensors or other objects, such as tf.Variable instances. Predefined collectionsBy default every tf.Variable gets placed in the following two collections: tf.GraphKeys.GLOBAL_VARIABLES variables that can be shared across multiple devices tf.GraphKeys.TRAINABLE_VARIABLE variables for which TensorFlow will calculate gradients If you don’t want a variable to be trainable, add it to the tf.GraphKeys.LOCAL_VARIABLES collection instead.The method is:12my_local = tf.get_variable("my_local", shape=(), collections=[tf.GraphKeys.LOCAL_VARIABLES]) or1my_non_trainable = tf.get_variable("my_non_trainable", shape=(), trainable=False) Custom collectionsThere is no need to explicitly create a collection.To add a variable (or any other object) to a collection:1tf.add_to_collection("my_collection_name", my_local) To retrieve a list of all the variables (or other objects) you’ve placed in a collection:1tf.get_collection("my_collection_name") Variable placementPlacement method:12with tf.device("/gpu:1"): v = tf.get_variable("v", [1]) It is particularly important for variables to be in the correct device in distributed settings.For this reason we provide tf.train.replica_device_setter, which can automatically place variables in parameter servers.1234567cluster_spec = &#123; "ps": ["ps0:2222", "ps1:2222"], "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]&#125;with tf.device(tf.train.replica_device_setter(cluster=cluster_spec)): v = tf.get_variable("v", shape=[20, 20]) # this variable is placed # in the parameter server # by the replica_device_setter Initializing variablesIn the low-level TensorFlow API, must explicitly initialize the variables by ourselves. that is, you are explicitly creating your own graphs and sessions) Most high-level frameworks automatically initialize variables for you tf.contrib.slim, tf.estimator.Estimator and Keras Explicit initialization is otherwise useful because it allows you not to rerun potentially expensive initializers when reloading a model from a checkpoint ? as well as allowing determinism when randomly-initialized variables are shared in a distributed setting.? tf.global_variables_initializer() To initialize all trainable variables To be called before training starts 12session.run(tf.global_variables_initializer())# Now all variables are initialized. To initialize variables yourself:1session.run(my_variable.initializer) ask which variables have still not been initialized:1print(session.run(tf.report_uninitialized_variables())) Noticeby default tf.global_variables_initializer does not specify the order in which variables are initialized.If you use a variable’s value while initializing another variable, use variable.initialized_value() instead of variable12v = tf.get_variable("v", shape=(), initializer=tf.zeros_initializer())w = tf.get_variable("w", initializer=v.initialized_value() + 1) Using variablesTo use the value of a tf.Variable in a TensorFlow graph, simply treat it like a normal tf.Tensor.To assign a value to a variable, use the methods such as assign, assign_add in the tf.Variable class.To force a re-read of the value of a variable after something has happened, you can use tf.Variable.read_value.12345678v = tf.get_variable("v", shape=(), initializer=tf.zeros_initializer())assignment = v.assign_add(1)assignment2 = v.assign_add(1)with tf.control_dependencies([assignment, assignment2]): w = v.read_value() # w is guaranteed to reflect v's value after the # assign_add operation.# v: 0# w: 2 tf.train.Optimizerhttps://www.tensorflow.org/api_docs/python/tf/train/Optimizer Saving and Restoringhttps://www.tensorflow.org/programmers_guide/variables#saving_and_restoring Sharing variableshttps://www.tensorflow.org/programmers_guide/variables#sharing_variables Tensorshttps://www.tensorflow.org/programmers_guide/tensors]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.contrib.learn 阅读历程]]></title>
    <url>%2F2017%2F08%2F18%2Ftf-contrib-learn%2F</url>
    <content type="text"><![CDATA[tf.contrib.learn 基本用法https://www.tensorflow.org/get_started/tflearn 基本过程 Load file containing training/test data into a TensorFlow Dataset Construct a neural network classifier Fit the model using the training data Evaluate the accuracy of the model Classify new samples/infer work Logging and Monitoring Basics with tf.contrib.learnhttps://www.tensorflow.org/get_started/monitorsTo track and evaluate progress in real time, When training a model.Without any logging, model training feels like a bit of a black box; you can’t see what’s happening as TensorFlow steps through gradient descent, get a sense of whether the model is converging appropriately, or audit to determine whether early stopping might be appropriate.One way to address this problem would be to split model training into multiple fit calls with smaller numbers of steps in order to evaluate accuracy more progressively. However, this is not recommended practice, as it greatly slows down model training.tf.contrib.learn offers another solution: a Monitor API designed to help you log metrics and evaluate your model while training is in progress.Enabling LoggingTensorFlow uses five different levels for log messages: DEBUG, INFO, WARN, ERROR, and FATAL.1tf.logging.set_verbosity(tf.logging.INFO) when tracking model training, you’ll want to adjust the level to INFO, which will provide additional feedback as fit operations are in progress.123INFO:tensorflow:loss = 1.18812, step = 1INFO:tensorflow:loss = 0.210323, step = 101INFO:tensorflow:loss = 0.109025, step = 201 With INFO-level logging, tf.contrib.learn automatically outputs training-loss metrics to stderr after every 100 steps. Configuring a ValidationMonitor for Streaming Evaluation tf.contrib.learn provides several high-level Monitors you can attach to your fit operations to further track metrics and/or debug lower-level TensorFlow operations during model training, including: Monitor Description CaptureVariable Saves a specified variable’s values into a collection at every n steps of training PrintTensor Logs a specified tensor’s values at every n steps of training SummarySaver Saves tf.Summary protocol buffers for a given tensor using a tf.summary.FileWriter at every n steps of training ValidationMonitor Logs a specified set of evaluation metrics at every n steps of training, and, if desired, implements early stopping under certain conditions Evaluating Every N Stepswhile logging training loss, you might also want to simultaneously evaluate against test data to see how well the model is generalizing.You can accomplish this by configuring a ValidationMonitor with the test data,, and setting how often to evaluate with every_n_steps. The default value of every_n_steps is 100.1234validation_monitor = tf.contrib.learn.monitors.ValidationMonitor( test_set.data, test_set.target, every_n_steps=50) ValidationMonitors rely on saved checkpoints to perform evaluation operations, so you’ll want to modify instantiation of the classifier to add a tf.contrib.learn.RunConfig that includes save_checkpoints_secs, which specifies how many seconds should elapse between checkpoint saves during training. Customizing the Evaluation Metrics with MetricSpecTo specify the exact metrics you’d like to run in each evaluation pass, you can add a metrics param to the ValidationMonitor constructor.metrics takes a dict of key/value pairs, where each key is the name you’d like logged for the metric, and the corresponding value is a MetricSpec object.MetricSpec constructor: metric_fn, prediction_key, label_key, weights_key. Early Stopping with ValidationMonitorIn addition to logging eval metrics, ValidationMonitors make it easy to implement early stopping when specified conditions are met, via three params: Param Description early_stopping_metric Metric that triggers early stopping (e.g., loss or accuracy) under conditions specified in early_stopping_rounds and early_stopping_metric_minimize. Default is “loss”. early_stopping_metric_minimize True if desired model behavior is to minimize the value of early_stopping_metric; False if desired model behavior is to maximize the value of early_stopping_metric. Default is True. early_stopping_rounds Sets a number of steps during which if the early_stopping_metric does not decrease (if early_stopping_metric_minimize is True) or increase (if early_stopping_metric_minimize is False), training will be stopped. Default is None, which means early stopping will never occur. Building Input Functionshttps://www.tensorflow.org/get_started/input_fn How to construct an input_fn to preprocess and feed data into your models.tf.contrib.learn supports using a custom input function (input_fn) to encapsulate the logic for preprocessing and piping data into your models. Anatomy of an input_fnthe basic skeleton for an input function:1234567def my_input_fn(): # Preprocess your data here... # ...then return 1) a mapping of feature columns to Tensors with # the corresponding feature data, and 2) a Tensor containing labels return feature_cols, labels The body of the input function contains the specific logic for preprocessing your input data, such as scrubbing out bad examples or feature scaling.Input functions must return the following two values containing the final feature and label data to be fed into your model (as shown in the above code skeleton): feature_cols (list of Tensors)A dict containing key/value pairs that map feature column names to Tensors (or SparseTensors) containing the corresponding feature data. labelsA Tensor containing your label (target) values: the values your model aims to predict. Converting Feature Data to TensorsFor continuous data, you can create and populate a Tensor using tf.constant:12feature_column_data = [1, 2.4, 0, 9.9, 3, 120]feature_tensor = tf.constant(feature_column_data) For sparse, categorical data (data where the majority of values are 0), you’ll instead want to populate a SparseTensor, which is instantiated with three arguments: dense_shapeThe shape of the tensor. Takes a list indicating the number of elements in each dimension. indices.The indices of the elements in your tensor that contain nonzero values. Takes a list of terms, where each term is itself a list containing the index of a nonzero element. valuesA one-dimensional tensor of values. Term i in values corresponds to term i in indices and specifies its value.123456sparse_tensor = tf.SparseTensor(indices=[[0,1], [2,4]], values=[6, 0.5], dense_shape=[3, 5])[[0, 6, 0, 0, 0] [0, 0, 0, 0, 0] [0, 0, 0, 0, 0.5]] Passing input_fn Data to Your Model1classifier.fit(input_fn=my_input_fn, steps=2000) Threading and QueuesQueues are a powerful mechanism for asynchronous computation using TensorFlow.A queue is a node in a TensorFlow graph. In particular, nodes can enqueue new items in to the queue, or dequeue existing items from the queue.N.B. Queue methods (such as q.enqueue(…)) must run on the same device as the queue. Incompatible device placement directives will be ignored when creating these operations. Queue usage overviewQueues, such as tf.FIFOQueue and tf.RandomShuffleQueue, are important TensorFlow objects for computing tensors asynchronously in a graph.A typical input architecture is to use a RandomShuffleQueue to prepare inputs for training a model: Multiple threads prepare training examples and push them in the queue. A training thread executes a training op that dequeues mini-batches from the queue. benefits in the Reading data Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.However,it is not always easy in Pythonbecause All threads must be able to stop together, exceptions must be caught and reported, and queues must be properly closed when stopping.so, TensorFlow provides two classes to help: tf.train.Coordinator and tf.train.QueueRunner.` tf.train.Coordinator helps multiple threads stop together, report exceptions to a program that waits for them to stop. QueueRunner create a number of threads cooperating to enqueue tensors in the same queue. Coordinatorhelps multiple threads stop togetherMethod: tf.train.Coordinator.should_stop: returns True if the threads should stop. tf.train.Coordinator.request_stop: requests that threads should stop. tf.train.Coordinator.join: waits until the specified threads have stopped. Basic Usage first create a Coordinator object, then create a number of threads that use the coordinator. The threads typically run loops that stop when coordinator.should_stop() returns True. Any thread can decide that the computation should stop by calling coordinator.request_stop(), to ask for all the threads to stop. To cooperate with the requests, each thread must check for coord.should_stop() on a regular basis.coord.should_stop() returns True as soon as coord.request_stop() has been called. 123456789101112131415161718# Thread body: loop until the coordinator indicates a stop was requested.# If some condition becomes true, ask the coordinator to stop.def MyLoop(coord): while not coord.should_stop(): ...do something... if ...some condition...: coord.request_stop()# Main thread: create a coordinator.coord = tf.train.Coordinator()# Create 10 threads that run 'MyLoop()'threads = [threading.Thread(target=MyLoop, args=(coord,)) for i in xrange(10)]# Start the threads and wait for all of them to stop.for t in threads: t.start()coord.join(threads) more detailhttps://www.tensorflow.org/api_docs/python/tf/train/Coordinator QueueRunnerThe QueueRunner class creates a number of threads that repeatedly run an enqueue op.These threads can use a coordinator to stop together.In addition, a queue runner runs a closer thread that automatically closes the queue if an exception is reported to the coordinator. basic usageStep 1: create Queue and add related ops to the queue. First create a queue (e.g. a tf.RandomShuffleQueue). Add ops that process examples and enqueue them in the queue. Create dequeue ops from the queue, and return data tensor . Use data tensor to build the Tensorflow graph.1234567example = ...ops to create one example...# Create a queue, and an op that enqueues examples one at a time in the queue.queue = tf.RandomShuffleQueue(...)enqueue_op = queue.enqueue(example)# Create a training graph that starts by dequeuing a batch of examples.inputs = queue.dequeue_many(batch_size)train_op = ...use 'inputs' to build the training part of the graph... 2. create QueueRunner and combine with Coordinator create a QueueRunner that will run a few threads to process and enqueue examples. . Launch the graph. Create a coordinator, launch the queue runner threads. Run the training loop, controlling termination with the coordinator. When done, ask the threads to stop. wait for all threads to actually stop. 123456789101112131415161718# Create a queue runner that will run 4 threads in parallel to enqueue# examples.qr = tf.train.QueueRunner(queue, [enqueue_op] * 4)# Launch the graph.sess = tf.Session()# Create a coordinator, launch the queue runner threads.coord = tf.train.Coordinator()enqueue_threads = qr.create_threads(sess, coord=coord, start=True)# Run the training loop, controlling termination with the coordinator.for step in xrange(1000000): if coord.should_stop(): break sess.run(train_op)# When done, ask the threads to stop.coord.request_stop()# And wait for them to actually do it.coord.join(enqueue_threads) Handling exceptionsThreads started by queue runners do more than just run the enqueue ops.tf.errors.OutOfRangeError exception, which is used to report that a queue was closed. A coordinator must similarly catch and report exceptions in its main loop.123456789101112try: for step in xrange(1000000): if coord.should_stop(): break sess.run(train_op)except Exception, e: # Report exceptions to the coordinator. coord.request_stop(e)finally: # Terminate as usual. It is safe to call `coord.request_stop()` twice. coord.request_stop() coord.join(threads) Reading datahttps://www.tensorflow.org/programmers_guide/reading_data Reading from filesA typical pipeline for reading records from files has the following stages: Step 1: Create string_input_producerCreates a FIFO queue for holding the filenames until the reader needs themArguments: filenames list. filename in list is either a constant string Tensor or tf.train.match_filenames_once function. shuffle and maximum number of epochs. A queue runner adds the whole list of filenames to the queue once for each epoch shuffling the filenames within an epoch if shuffle=True. This procedure provides a uniform sampling of files, so that examples are not under- or over- sampled relative to each other. The queue runner works in a thread separate from the reader that pulls filenames from the queue, so the shuffling and enqueuing process does not block the reader. 1filename_queue = tf.train.string_input_producer(["file0.csv", "file1.csv"]) Step 2: create a ReaderA. Select the reader that matches your input file formatB. Then pass the filename queue to the reader’s read method. The read method outputs a key and a scalar string value. key: identifying the file and record (useful for debugging if you have some weird records) Each execution of read reads a single line from the file. C. Use one (or more) of the decoder and conversion ops to decode this string into the tensors that make up an example. Step 3: call tf.train.start_queue_runnerscall tf.train.start_queue_runners to populate the queue before you call run or eval to execute the read. Standard TensorFlow format -TFRecordsmore details:https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_detailshttps://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/convert_to_records.py PreprocessingYou can then do any preprocessing of these examples you want.Examples include normalization of your data, picking a random slice, adding noise or distortionsmore details:https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10/cifar10_input.py BatchingAt the end of the pipeline we use another queue to batch together examples for training, evaluation, or inference. For this we use a queue that randomizes the order of examples, using the tf.train.shuffle_batch.1234567891011121314151617181920212223def read_my_file_format(filename_queue): reader = tf.SomeReader() key, record_string = reader.read(filename_queue) example, label = tf.some_decoder(record_string) processed_example = some_processing(example) return processed_example, labeldef input_pipeline(filenames, batch_size, num_epochs=None): filename_queue = tf.train.string_input_producer( filenames, num_epochs=num_epochs, shuffle=True) example, label = read_my_file_format(filename_queue) # min_after_dequeue defines how big a buffer we will randomly sample # from -- bigger means better shuffling but slower start up and more # memory used. # capacity must be larger than min_after_dequeue and the amount larger # determines the maximum we will prefetch. Recommendation: # min_after_dequeue + (num_threads + a small safety margin) * batch_size min_after_dequeue = 10000 capacity = min_after_dequeue + 3 * batch_size example_batch, label_batch = tf.train.shuffle_batch( [example, label], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue) return example_batch, label_batch multiple reader and one single filename queueIf we need more parallelism or shuffling of examples between files, use multiple reader instances using the tf.train.shuffle_batch_join. For example:1234567891011121314def read_my_file_format(filename_queue): # Same as abovedef input_pipeline(filenames, batch_size, read_threads, num_epochs=None): filename_queue = tf.train.string_input_producer( filenames, num_epochs=num_epochs, shuffle=True) example_list = [read_my_file_format(filename_queue) for _ in range(read_threads)] min_after_dequeue = 10000 capacity = min_after_dequeue + 3 * batch_size example_batch, label_batch = tf.train.shuffle_batch_join( example_list, batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue) return example_batch, label_batch We still only use a single filename queue that is shared by all the readers. That way we ensure that the different readers use different files from the same epoch until all the files from the epoch have been started. It is also usually sufficient for a single thread to fill the filename queue.) How many threads do you need?the tf.train.shuffle_batch* functions add a summary to the graph that indicates how full the example queue is. If you have enough reading threads, that summary will stay above zero. Creating threads to prefetch using QueueRunner objectsmany of the tf.train functions listed above add tf.train.QueueRunner objects to your graph.These require that you call tf.train.start_queue_runners before running any training or inference steps, or it will hang forever.This is best combined with a tf.train.Coordinator to cleanly shut down these threads when there are errors.The recommended code pattern:123456789101112131415161718192021222324252627# Create the graph, etc.init_op = tf.global_variables_initializer()# Create a session for running operations in the Graph.sess = tf.Session()# Initialize the variables (like the epoch counter).sess.run(init_op)# Start input enqueue threads.coord = tf.train.Coordinator()threads = tf.train.start_queue_runners(sess=sess, coord=coord)try: while not coord.should_stop(): # Run training steps or whatever sess.run(train_op)except tf.errors.OutOfRangeError: print('Done training -- epoch limit reached')finally: # When done, ask the threads to stop. coord.request_stop()# Wait for threads to finish.coord.join(threads)sess.close() Summarizeread data processFirst we create the graph.It will have a few pipeline stages that are connected by queues. The first stage: generate filenames to read and enqueue them in the filename queue. The second stage: consumes filenames (using a Reader), produces examples, and enqueues them in an example queue. Depending on how you have set things up, you may actually have a few independent copies of the second stage (in ther word, several readers), so that you can read from multiple files in parallel. At the end is an enqueue operation, which enqueues into a queue that the next stage dequeues from. We want to start threads running these enqueuing operations, so that our training loop can dequeue examples from the example queue.Method: add a tf.train.QueueRunner to the graph using the tf.train.add_queue_runner function. Each QueueRunner is responsible for one stage, and holds the list of enqueue operations that need to be run in threads. Once the graph is constructed, the tf.train.start_queue_runners function asks each QueueRunner in the graph to start its threads running the enqueuing operations. If all goes well, you can now run your training steps and the queues will be filled by the background threads.If you have set an epoch limit, at some point an attempt to dequeue examples will get an tf.errors.OutOfRangeError(Equal to EOS/EOF). The last ingredient is the tf.train.Coordinator. This is responsible for letting all the threads know if anything has signalled a shut down. Most commonly this would be because an exception was raised, for example one of the threads got an error when running some operation (or an ordinary Python exception). Filtering records or producing multiple examples per recordInstead of examples with shapes [x, y, z], you will produce a batch of examples with shape [batch, x, y, z]. The batch size can be 0 if you want to filter this record out (maybe it is in a hold-out set?), or bigger than 1 if you are producing multiple examples per record. Then simply set enqueue_many=True when calling one of the batching functions (such as shuffle_batch or shuffle_batch_join). Using the Datasethttps://www.tensorflow.org/programmers_guide/datasets Two API abstractionstf.contrib.data.Datasetcontains a sequence of elementsThere are two distinct ways to create a dataset: Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more tf.Tensor objects. Applying a transformation (e.g. Dataset.batch()) constructs a dataset from one or more tf.contrib.data.Dataset objects. tf.contrib.data.IteratorProvides the main way to extract elements from a dataset.The operation returned by Iterator.get_next() yields the next element of a Dataset when executed, and typically acts as the interface between input pipeline code and your model.For more sophisticated uses, the Iterator.initializer operation enables you to reinitialize and parameterize an iterator with different datasets, so that you can, for example, iterate over training and validation data multiple times in the same program. Basic mechanicsDefine a source construct a Dataset from some tensors in memory use tf.contrib.data.Dataset.from_tensors() use tf.contrib.data.Dataset.from_tensor_slices() construct a Dataset from your input data are on disk in the recommend TFRecord format use tf.contrib.data.TFRecordDataset transform a Dataset into a new Dataset apply per-element transformations such as Dataset.map()(to apply a function to each element), and multi-element transformations such as Dataset.batch(). The most common way to consume values from a Dataset make an iterator object that provides access to one element of the dataset at a time. for example, by calling Dataset.make_one_shot_iterator(). A tf.contrib.data.Iterator provides two operations: Iterator.initializer enables you to (re)initialize the iterator’s state Iterator.get_next() returns tf.Tensor objects that correspond to the symbolic next element. Depending on your use case, you might choose a different type of iterator, and the options are outlined below. Dataset structureA dataset comprises elements that each have the same structure, called components. Each component has a tf.DType and a tf.TensorShape. The Dataset.output_types and Dataset.output_shapes properties allow you to inspect the inferred types and shapes of each component of a dataset element. The nested structure of these properties map to the structure of an element, which may be a single tensor, a tuple of tensors, or a nested tuple of tensors. 12345678910111213dataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))print(dataset1.output_types) # ==&gt; "tf.float32"print(dataset1.output_shapes) # ==&gt; "(10,)"dataset2 = tf.contrib.data.Dataset.from_tensor_slices( (tf.random_uniform([4]), tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))print(dataset2.output_types) # ==&gt; "(tf.float32, tf.int32)"print(dataset2.output_shapes) # ==&gt; "((), (100,))"dataset3 = tf.contrib.data.Dataset.zip((dataset1, dataset2))print(dataset3.output_types) # ==&gt; (tf.float32, (tf.float32, tf.int32))print(dataset3.output_shapes) # ==&gt; "(10, ((), (100,)))" use collections.namedtuple or a dictionary mapping strings to tensors to represent a single element of a Dataset. 12345dataset = tf.contrib.data.Dataset.from_tensor_slices( &#123;"a": tf.random_uniform([4]), "b": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)&#125;)print(dataset.output_types) # ==&gt; "&#123;'a': tf.float32, 'b': tf.int32&#125;"print(dataset.output_shapes) # ==&gt; "&#123;'a': (), 'b': (100,)&#125;" The Dataset transformations support datasets of any structure. When using the Dataset.map(), Dataset.flat_map(), and Dataset.filter() transformations, which apply a function to each element, the element structure determines the arguments of the function: 123456dataset1 = dataset1.map(lambda x: ...)dataset2 = dataset2.flat_map(lambda x, y: ...)# Note: Argument destructuring is not available in Python 3.dataset3 = dataset3.filter(lambda x, (y, z): ...) Creating an iteratorAfter building a Dataset, the next step is to create an Iterator to access elements from that dataset.The Dataset API currently supports three kinds of iterator, in increasing level of sophistication: one-shot supports iterating once through a dataset no need for explicit initialization handle almost all of the cases that the existing queue-based input pipelines support 1234567dataset = tf.contrib.data.Dataset.range(100)iterator = dataset.make_one_shot_iterator()next_element = iterator.get_next()for i in range(100): value = sess.run(next_element) assert i == value initializable must run an explicit iterator.initializer operation before using it. enables you to parameterize the definition of the dataset using one or more tf.placeholder() tensors that can be fed when you initialize the iterator. reinitializable can be initialized from multiple different Dataset objects A reinitializable iterator is defined by its structure. For example, training input pipeline uses random perturbations and validation input pipeline uses on unmodified data. These pipelines will typically use different Dataset objects that have the same structure (i.e. the same types and compatible shapes for each component). 123456789101112131415161718192021222324252627# Define training and validation datasets with the same structure.training_dataset = tf.contrib.data.Dataset.range(100).map( lambda x: x + tf.random_uniform([], -10, 10, tf.int64))validation_dataset = tf.contrib.data.Dataset.range(50)# A reinitializable iterator is defined by its structure. We could use the# `output_types` and `output_shapes` properties of either `training_dataset`# or `validation_dataset` here, because they are compatible.iterator = Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)next_element = iterator.get_next()training_init_op = iterator.make_initializer(training_dataset)validation_init_op = iterator.make_initializer(validation_dataset)# Run 20 epochs in which the training dataset is traversed, followed by the# validation dataset.for _ in range(20): # Initialize an iterator over the training dataset. sess.run(training_init_op) for _ in range(100): sess.run(next_element) # Initialize an iterator over the validation dataset. sess.run(validation_init_op) for _ in range(50): sess.run(next_element) feedable used together with tf.placeholder to select what Iterator to use in each call to tf.Session.run offers the same functionality as a reinitializable iterator not require to initialize the iterator from the start of a dataset when you switch between iterators use tf.contrib.data.Iterator.from_string_handle to define a feedable iterator that allows you to switch between the two datasets 123456789101112131415161718192021222324252627282930313233343536# Define training and validation datasets with the same structure.training_dataset = tf.contrib.data.Dataset.range(100).map( lambda x: x + tf.random_uniform([], -10, 10, tf.int64)).repeat()validation_dataset = tf.contrib.data.Dataset.range(50)# A feedable iterator is defined by a handle placeholder and its structure. We# could use the `output_types` and `output_shapes` properties of either# `training_dataset` or `validation_dataset` here, because they have# identical structure.handle = tf.placeholder(tf.string, shape=[])iterator = tf.contrib.data.Iterator.from_string_handle( handle, training_dataset.output_types, training_dataset.output_shapes)next_element = iterator.get_next()# You can use feedable iterators with a variety of different kinds of iterator# (such as one-shot and initializable iterators).training_iterator = training_dataset.make_one_shot_iterator()validation_iterator = validation_dataset.make_initializable_iterator()# The `Iterator.string_handle()` method returns a tensor that can be evaluated# and used to feed the `handle` placeholder.training_handle = sess.run(training_iterator.string_handle())validation_handle = sess.run(validation_iterator.string_handle())# Loop forever, alternating between training and validation.while True: # Run 200 steps using the training dataset. Note that the training dataset is # infinite, and we resume from where we left off in the previous `while` loop # iteration. for _ in range(200): sess.run(next_element, feed_dict=&#123;handle: training_handle&#125;) # Run one pass over the validation dataset. sess.run(validation_iterator.initializer) for _ in range(50): sess.run(next_element, feed_dict=&#123;handle: validation_handle&#125;) Consuming values from an iteratorIf the iterator reaches the end of the dataset, executing the Iterator.get_next() operation will raise a tf.errors.OutOfRangeError.Must initialize it again if you want to use it further.If each element of the dataset has a nested structure, the return value of Iterator.get_next() will be one or more tf.Tensor objects in the same nested structure: 1234567891011121314dataset1 = tf.contrib.data.Dataset.range(50)dataset2 = tf.contrib.data.Dataset.range(50)dataset3 = tf.contrib.data.Dataset.zip((dataset1, dataset2))iterator = dataset3.make_initializable_iterator()sess = tf.Session()sess.run(iterator.initializer)next1, next2 = iterator.get_next()print(sess.run([next1]))print(sess.run([next2]))# evaluating any of next1, next2, or next3 will advance the iterator for all componentsprint(sess.run(iterator.get_next())# A typical consumer of an iterator will include all components in a single expression.print(sess.run([next1, next2 + 1])) Notice: evaluating any of next1, next2, or next3 will advance the iterator for all components A typical consumer of an iterator will include all components in a single expression. Reading input dataConsuming NumPy arraysDataset + tf.placeholder() + Iterator + feed 123456789101112131415161718# Load the training data into two NumPy arrays, for example using `np.load()`.with np.load("/var/data/training_data.npy") as data: features = data["features"] labels = data["labels"]# Assume that each row of `features` corresponds to the same row as `labels`.assert features.shape[0] == labels.shape[0]features_placeholder = tf.placeholder(features.dtype, features.shape)labels_placeholder = tf.placeholder(labels.dtype, labels.shape)dataset = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))# [Other transformations on `dataset`...]dataset = ...iterator = dataset.make_initializable_iterator()sess.run(iterator.initializer, feed_dict=&#123;features_placeholder: features, labels_placeholder: labels&#125;) Consuming TFRecord datatf.contrib.data.TFRecordDataset if you have two sets of files for training and validation tf.placeholder + iterator 1234567891011121314151617filenames = tf.placeholder(tf.string, shape=[None])dataset = tf.contrib.data.TFRecordDataset(filenames)dataset = dataset.map(...) # Parse the record into tensors.dataset = dataset.repeat() # Repeat the input indefinitely.dataset = dataset.batch(32)iterator = dataset.make_initializable_iterator()# You can feed the initializer with the appropriate filenames for the current# phase of execution, e.g. training vs. validation.# Initialize `iterator` with training data.training_filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]sess.run(iterator.initializer, feed_dict=&#123;filenames: training_filenames&#125;)# Initialize `iterator` with validation data.validation_filenames = ["/var/data/validation1.tfrecord", ...]sess.run(iterator.initializer, feed_dict=&#123;filenames: validation_filenames&#125;) Consuming text datatf.contrib.data.TextLineDataset Given one or more filenames, produce one string-valued element per line of those files. can use tf.placeholder(tf.string) remove unrelated lines using the Dataset.skip() and Dataset.filter(), more than one filename use Dataset.flat_map() in addition. Preprocessing data with Dataset.map()Dataset.map() transformation works in each item in dataset. Parsing tf.Example protocol buffer messagesFor TFRecordDataset and TFRecord-format fileEach tf.train.Example record contains one or more “features”, and the input pipeline typically converts these features into tensors.12345678910111213# Transforms a scalar string `example_proto` into a pair of a scalar string and# a scalar integer, representing an image and its label, respectively.def _parse_function(example_proto): features = &#123;"image": tf.FixedLenFeature((), tf.string, default_value=""), "label": tf.FixedLenFeature((), tf.int32, default_value=0)&#125; parsed_features = tf.parse_single_example(example_proto, features) return parsed_features["image"], parsed_features["label"]# Creates a dataset that reads all of the examples from two files, and extracts# the image and label features.filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.contrib.data.TFRecordDataset(filenames)dataset = dataset.map(_parse_function) Decoding image data and resizing itnecessary to convert images of different sizes to a common size, so that they may be batched into a fixed size. 12345678910111213141516# Reads an image from a file, decodes it into a dense tensor, and resizes it# to a fixed shape.def _parse_function(filename, label): image_string = tf.read_file(filename) image_decoded = tf.image.decode_image(image_string) image_resized = tf.image.resize_images(image_decoded, [28, 28]) return image_resized, label# A vector of filenames.filenames = tf.constant(["/var/data/image1.jpg", "/var/data/image2.jpg", ...])# `labels[i]` is the label for the image in `filenames[i].labels = tf.constant([0, 37, ...])dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))dataset = dataset.map(_parse_function) Applying arbitrary Python logic with tf.py_func()sometimes useful to call upon external Python libraries when parsing your input data12345678910111213141516171819202122import cv2# Use a custom OpenCV function to read the image, instead of the standard# TensorFlow `tf.read_file()` operation.def _read_py_function(filename, label): image_decoded = cv2.imread(image_string, cv2.IMREAD_GRAYSCALE) return image_decoded, label# Use standard TensorFlow operations to resize the image to a fixed shape.def _resize_function(image_decoded, label): image_decoded.set_shape([None, None, None]) image_resized = tf.image.resize_images(image_decoded, [28, 28]) return image_resized, labelfilenames = ["/var/data/image1.jpg", "/var/data/image2.jpg", ...]labels = [0, 37, 29, 1, ...]dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))dataset = dataset.map( lambda filename, label: tf.py_func( _read_py_function, [filename, label], [tf.uint8, label.dtype]))dataset = dataset.map(_resize_function) Batching dataset elementsSimple batchingDataset.batch() transformationbatching stacks n consecutive elements of a dataset into a single element123456789101112inc_dataset = tf.contrib.data.Dataset.range(100)dec_dataset = tf.contrib.data.Dataset.range(0, -100, -1)dataset = tf.contrib.data.Dataset.zip((inc_dataset, dec_dataset))batched_dataset = dataset.batch(4)iterator = batched_dataset.make_one_shot_iterator()next_element = iterator.get_next()sess = tf.Session()print(sess.run(next_element)) # ==&gt; ([0, 1, 2, 3], [ 0, -1, -2, -3])print(sess.run(next_element)) # ==&gt; ([4, 5, 6, 7], [-4, -5, -6, -7])print(sess.run(next_element)) # ==&gt; ([8, 9, 10, 11], [-8, -9, -10, -11]) Batching tensors with paddingDataset.padded_batch() transformation123456789101112dataset = tf.contrib.data.Dataset.range(100)dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))dataset = dataset.padded_batch(4, padded_shapes=[None])iterator = dataset.make_one_shot_iterator()next_element = iterator.get_next()print(sess.run(next_element)) # ==&gt; [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]print(sess.run(next_element)) # ==&gt; [[4, 4, 4, 4, 0, 0, 0], # [5, 5, 5, 5, 5, 0, 0], # [6, 6, 6, 6, 6, 6, 0], # [7, 7, 7, 7, 7, 7, 7]] The Dataset.padded_batch() transformation allows you to set different padding for each dimension of each component Training workflowsProcessing multiple epochsDataset API offers two main ways to process multiple epochs of the same data. The simplest way: Dataset.repeat(num_repeat) with no arguments will repeat the input indefinitely without signaling the end of one epoch and the beginning of the next epoch 12345filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.contrib.data.TFRecordDataset(filenames)dataset = dataset.map(...)dataset = dataset.repeat(10)dataset = dataset.batch(32) loop + catch the tf.errors.OutOfRangeError, If want to receive a signal at the end of each epoch, to use a training loop that catches the tf.errors.OutOfRangeError. No need repeat. 1234567891011121314151617filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.contrib.data.TFRecordDataset(filenames)dataset = dataset.map(...)dataset = dataset.batch(32)iterator = dataset.make_initializable_iterator()next_element = iterator.get_next()# Compute for 100 epochs.for _ in range(100): sess.run(iterator.initializer) while True: try: sess.run(next_element) except tf.errors.OutOfRangeError: break # [Perform end-of-epoch calculations here.] Randomly shuffling input dataDataset.shuffle() tansformation Randomly shuffles the input dataset using a similar algorithm to tf.RandomShuffleQueue maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer. 123456filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.contrib.data.TFRecordDataset(filenames)dataset = dataset.map(...)dataset = dataset.shuffle(buffer_size=10000)dataset = dataset.batch(32)dataset = dataset.repeat() Using high-level APIstf.train.MonitoredTrainingSessiontf.train.MonitoredTrainingSession Simplifies many aspects of running TensorFlow in a distributed setting. uses the tf.errors.OutOfRangeError to signal that training has completedwhen used with Dataset API, recommend using Dataset.make_one_shot_iterator() Demo:12345678910111213141516filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.contrib.data.TFRecordDataset(filenames)dataset = dataset.map(...)dataset = dataset.shuffle(buffer_size=10000)dataset = dataset.batch(32)dataset = dataset.repeat(num_epochs)iterator = dataset.make_one_shot_iterator()next_example, next_label = iterator.get_next()loss = model_function(next_example, next_label)training_op = tf.train.AdagradOptimizer(...).minimize(loss)with tf.train.MonitoredTrainingSession(...) as sess: while not sess.should_stop(): sess.run(training_op) tf.estimator.Estimatorwhen using a Dataset in the input_fn of tf.estimator.Estimator, recommend using Dataset.make_one_shot_iterator()Demo:12345678910111213141516171819202122232425262728293031323334def dataset_input_fn(): filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"] dataset = tf.contrib.data.TFRecordDataset(filenames) # Use `tf.parse_single_example()` to extract data from a `tf.Example` # protocol buffer, and perform any additional per-record preprocessing. def parser(record): keys_to_features = &#123; "image_data": tf.FixedLenFeature((), tf.string, default_value=""), "date_time": tf.FixedLenFeature((), tf.int64, default_value=""), "label": tf.FixedLenFeature((), tf.int64, default_value=tf.zeros([], dtype=tf.int64)), &#125; parsed = tf.parse_single_example(record, keys_to_features) # Perform additional preprocessing on the parsed data. image = tf.decode_jpeg(parsed["image_data"]) image = tf.reshape(image, [299, 299, 1]) label = tf.cast(parsed["label"], tf.int32) return &#123;"image_data": image, "date_time": parsed["date_time"]&#125;, label # Use `Dataset.map()` to build a pair of a feature dictionary and a label # tensor for each example. dataset = dataset.map(parser) dataset = dataset.shuffle(buffer_size=10000) dataset = dataset.batch(32) dataset = dataset.repeat(num_epochs) iterator = dataset.make_one_shot_iterator() # `features` is a dictionary in which each value is a batch of values for # that feature; `labels` is a batch of labels. features, labels = iterator.get_next() return features, labels laterhttps://stackoverflow.com/questions/41175011/tf-contrib-learn-tutorial-deprecation-warning]]></content>
      <tags>
        <tag>tensorflow</tag>
        <tag>tf.contrib.learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Research Plan]]></title>
    <url>%2F2017%2F06%2F10%2Frs2%2F</url>
    <content type="text"><![CDATA[Objective To propose a new sequence alignment method based on neural network. ProblemGiven an sequence pair set S. In set S, each sequence pair &lt; ${Seq}_A$, ${Seq}_B$&gt; has best alignment form $Align({Seq}_A,{Seq}_B)$. For example, pair &lt; ${Seq}_A$, ${Seq}_B$&gt;: LDRCVPKFWTLHKN LEKCWTMNMCQKN the best alignment form is: LDRCVPKFWTL——HKN LEKC——WTMNMCQKN For a new sequence pair not in S, how to predict its best alignment form. Further Formalize ProblemWe change this problem to another scenery. There are two queue QueueA and QueueB. SeqA is in QueueA, SeqB is in QueueB. There are three allowed action to be adopted at two queue: M, I,and D. which means,123456M: dequeue QueueA and QueueBI: only dequeue QueueAD: only dequeue QueueB each time adopts a stategy. when two queue is empty, we get a strategy sequence. then problem is change to this statement. Given an sequence pair set S. In set S, each sequence pair &lt; ${Seq}_A$, ${Seq}_B$&gt; has best dequeue strategy process $Align({Seq}_A,{Seq}_B)$. For SeqA and SeqB above, the best dequeue strategy process is: MMMMIIIIMMMMMDDDDMMM For a new sequence pair not in S, how to predict its best strategy process. MotivationThe strength of deep learning is feature extraction, metric space mapping and big data processing. Consider the problem complexity of sequence alignment, only simple deep learning method is not enough. Seq2seq model provides a simple end-to-end architecture to deal with a multi-step decision problem. Furthermore, Seq2seq model can be easily modified and expanded. By formalizing sequence alignment to strategy sequence, we adopt seq2seq neural network to model sequence alignment problem. MethodDataSetSCOPe2.6 database proteins alignments_family alignments_superfamily alignments_fold scope20_2.06 7659 16194 66229 133900 scope40_2.06 13760 98558 305553 526136 scope70_2.06 21132 314039 907374 1470385 scope95_2.06 28010 1099425 2963970 4170984 scope100_2.06 69590 6060416 16688439 23215716 scopeall_2.06 216248 69574992 172157027 239906177 Model Textual EncoderGiven two input sequences $X_1=(a_1,a_2,…,a_{T_1}),X_2=(b_1,b_2,…,b_{T_2})$by Bi-LSTM and Multi_LSTM, get encodes of input sequencesIn another perspective, we can consider encoders as memory of input. $A^{X_1}=Encoder(a_1,a_2,…,a_{T_1})=\{h_1^{X_1},h_2^{X_1},…,h_{T1}^{X_1}\}$ $A^{X_2}=Encoder(b_1,b_2,…,b_{T_2})=\{h_1^{X_2},h_2^{X_2},…,h_{T2}^{X_2}\}$ Joint mutimodal representationMutimodal Vector ConcatFor first try, we consider to concat two input memory to get a joint representation. A = concat(A^{X_1},A^{X_2})Bilinear pooling.Bilinear models (Tenenbaum and Freeman, 2000) take the outer product of two vectors $x ∈ R^{n_1}$ and $q ∈ R^{n_2}$ and learn a model $W$ (here linear), i.e.$z = W [x ⊗ q]$, where $⊗$ denotes the outer product$(xq^T)$ and $[]$ denotes linearizing the matrix in a vector.However, the high dimensional representation leads to an infeasible number of parameters to learn in $W$.We thus need a method that projects the outer product to a lower dimensional space and also avoids computing the outer product directly. Multimodal Compact Bilinear Pooling (MCB)This allows us to project the outer product to a lower dimensional space, which reduces the number of parameters in W. To avoid computing the outer product explicitly, Pham and Pagh (2013) showed that the count sketch of the outer product of two vectors can be expressed as convolution of both count sketches: $Ψ(x ⊗ q, h, s) = Ψ(x, h, s) ∗ Ψ(q, h, s)$, where ∗ is the convolution operator. Additionally, the convolution theorem states that convolution in the time domain is equivalent to element-wise product in the frequency domain. The convolution $x’∗ q’$ can be rewritten as ${FFT}^{−1}(FFT(x’) \centerdot FFT(q’))$, where $\centerdot$ refers to element-wise product. These ideas are formalized in Algorithm 1, which is based on the Tensor Sketch algorithm of Pham and Pagh (2013). We invoke the algorithm with $A^{X_1} = x$ and $A^{X_2} = q$. We note that this easily extends and remains efficient for more than two multi-modal inputs as the combination happens as element-wise product. A=MCB(A^{X_1},A^{X_2})=\{h_1,h_2,...,h_{T}\}AttentionAt every time-step, the attention mechanism computes a context vector $c^t$, given the current decoder state $s_t$ and memory set $A^m$.Firstly, we compute attention weights $α_t = softmax(v^Ttanh(W_1A + W_2s_t + b))$.Then, the context vector is obtained with the following weighted sum : $c_t =\sum\limits ^{|A|}_{i=1} α_{ti}h_i$ DecoderThe decoder produces an output sequence $Y = (y1, y2, …, y_{T_0} )$, is initialized by $s0 = tanh(W_{init}h^t_T + b_{init})$ where $h^t_T$ is the textual encoder’s last state.The next decoder states are obtained as follows:$s_t, o_t = Decoder(s_{t−1}, W_{in}[y_{t−1}: c_{t−1}])$During training, $y_{t−1}$ is the ground truth symbol in the sentence while $c_{t−1}$ is the previous attention vector computed by the attention model. The current attention vector $c_t$, concatenated with the output $o_t$, is used to compute a new vector $o’_t = W_{proj} [o_t; c_t] + b{proj} $.The probability distribution over the target vocabulary is computed by the equation :$p(y_t|y_{t−1}, s_{t−1}, c_{t−1}, A^{X_1}, A^{X_2}) = softmax(W_{out}o’_t + b_{out})$ Loss FunctionCross EntropyH(p,q)=− \sum_x p(x)\log q(x)Evaluate the difference between the predicted probability distribution and the true distribution of the current training. OptimizerAdamAdam(Adaptive Moment Estimation): To use the first order moment estimation and second order moment estimation of the gradient to dynamically adjust the learning rate of each parameter.Commonly used together with RNN. ResultsExperiment 1DataSetsequences in SCOPe100_2.06, and sequence are aligned between family. Furthermore, the length of sequences are less than 50. Training Set: 50952Evaluation Set: 6369Testing Set: 6369Training:Evaluation:Testing=8:1:1 Data FormatInput1: _AG AGK GKK KKL KLF LFK FKC KCN CNE NEC ECK CKK KKT KTF TFT FTQ TQS QSS SSS SSL SLT LTV TVH VHQ HQR QRI RIH IHT HTG TGE GEK EK_ Input2: _VK VKP KPY PYG YGC GCS CSE SEC ECG CGK GKA KAF AFR FRS RSK SKS KSY SYL YLI LII IIH IHM HMR MRT RTH TH_ Target: D M D M M M M M M M M M M M M M M M M M M M M M M M M M D D D D Multimodel Joint StyleMutimodal Vector Concatconcat two input memory to get a joint representationFor reason: easy to achievement, and use less memory. EffectHorizontal axis coordinates: stepsVertical axis coordinates: loss Analysis: Fluctuation of losses illustrate different batch data has different characteristic and current training model result havn’t catch it. Overall decline of traing loss and evaluation loss illustrate current training model result learning something from traing dataset. Test Result: CASE 1:I D M M M M M M M M M M M M M M M M M M M M M M M M M M MI D M M M M M M M M M M M M M M M M M M M M M M M M M M M CASE 2:I I I M M M M M M M M M M M M M M I M M M M M M M M M M M M M M M D M M M M M M M M M M MI I I M M M M M M M M M M M M M M I M M M M M M M M M M M M M M M M D M M M M M M M M M M CASE 3:D D D D D D D I I I I M M I M I I I M M M M I M M M M M I M M M M M M M M M M M M D D M M M M I I I I I ID I I I M M D D D D M M M M I I I M M M M M M I I M M M M M M M M M M M M M M D M M M M M I I I I I CASE 4:D I I M M M D D D M M M D D M M M M M D D D D D I I M M M M M M M I I M M M M I M M M M M D M M D D D MI I I M M M D D D D D M M M D M M M D D D D D D M I I I M M M M M M M M M M M M I I M M M M M M D M D D D EvaluatioinEasy case work.Too long sequence doesn’t work well. For the difference between IDM-style align result and the difference between straightforward-style align result, the relationship between the two foramt is not so straight-forward. Problelma. How to evaluate result is still a problem? not just a loss. Or is there a better loss. add some metrics for validation results build evaluation metircs for final results According to the alignment results of testing data， build structure, compare testing structure with structurre built from alignment resutls of tmalign. b.Memory footprint and data loading. Improvementadd metrics for validation process.metrics is used to measure the difference between two sequence alignments. metric formula Modeller score $S_{mod}=N_{correct}/L_{align}$ Maxsub score $S_{maxsub}=N_{correct}/min(L_q,Lp)$ balance score $S_{bal}=(S_{mod}+S_{maxsub})/2$ ResultsConclusion]]></content>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conference]]></title>
    <url>%2F2017%2F06%2F05%2Fconference%2F</url>
    <content type="text"><![CDATA[NIPS NIPS 2016生成模型highlightGANPlug &amp; Play Generative Networks - Ian Goodfellow VAEVariational Inference- David Blei最有影响的是重新参数化（reparameterization）的技巧，该技巧可以通过随机变量实现反向传播，同时也推动了变自编码器上最新进展。 SAGASAGA VS BFGS - Francis Bachhttps://arxiv.org/abs/1407.0202 othersExponential Family Embeddings一种多类型数据的全新强大的嵌入（embedding）技术，带来了用户使用嵌入技术评估数据的可能性。 Unsupervised Learning for Physical Interaction through Video Prediction利用机器人的推动动作的数据来规划可能的未来。传统的代理（agent）学习算法严重依赖于监督，而这种类型的方法或许是机器人和类似领域未来的新方法。 Improving Variational Inference with Inverse Autoregressive Flow结合了变分推断的最新进展和自动回归网络（autoregressive network）的一些想法，得出更好的编码模型。 强化学习highlightValue Iteration Network该论文的主要创新在于其模型包含了一个可微分的「规划模块（planning module），这让网络可以做出规划并更好地泛化到其它从未见过的领域。 Sequential Neural Models with Stochastic Layers将 状态空间模型（State Space Model）的想法和 RNN 结合起来，充分利用了两个领域的最好的东西。 Phased LSTMs将「time gate」添加到了 LSTM 中，这显著改善了针对长序列数据的优化和表现。 Bayesian Intermittent Demand Forecasting for Large Inventories讨论了针对大型库存的贝叶斯间断需求预测 Fast and Provably Good Seedings for k-MeansK-means 是许多数据科学应用的核心算法。不过，找到好的聚类中心（cluster centers）常常要依赖良好的初始化。Olivier Bachem 在论文《Fast and Provably Good Seedings for k-Means》中表明，他们可以获得良好的 centroid seeds，速度比当前最佳的算法（k-Means++）快几个数量级。更妙的是，他们还有代码，「pip install kmc2」= g2g。 othersAttend, Infer, Repeat: Fast Scene Understanding with Generative Models提出了一种极具启发性的理解图像中场景的方法。使用贝叶斯和变分推理（Bayesian and Variational Inference），该论文的作者构建了一个可以无需任何监督就能理解图像中的数字、地址和物体类型的模型。这引起了较大的关注，因为他们的模型可以在训练样本之外的分布上进行推理/推导。当然，该模型确实需要一些特别的需求，但它们也提供了新的有趣的研究探索路径。 DeepMath—Deep Sequence Models for Premise Selection提出的深度学习方法可以持续不断突破新的领域。一个来自 Google Research 的研究团队（包括 François Chollet and Geoffrey Irving）展示了世界上第一个使用深度学习进行自动理论证明（automated theorem proving）的案例。这项成果有助于加速系统的正确性证明，并可替代对该领域的专家所设计的特征的需求（其与自然语言有类似但也不同的结构）。它们可以自动选择与推理过程中的当前状态相关的操作运算，这个过程可以被扩展到其它领域，是一个非常激动人心的研究方向。 # Supervised Word Mover’s Distance词嵌入帮助改变了许多自然语言处理任务，去年《Word Movers Distance》提供一种使用它们的嵌入在不同文档进行摘要的方法。对监督任务（比如，文本分类）而言，这可以更进一步。《Supervised Word Mover’s Distance》提出了可以执行仿射变换（affine transformation）和重新调整权重（re-weightings）的方法来提供分类，实现了有效的当前最佳的表现。 应用:机器人与汽车Learning to Poke by Poking: Experiential Learning of Intuitive Physics。他们使用了几百个小时数据（让机器人通过戳的动作来移动物体获得的）搭建了一个系统，机器人可以四处移动物体即使它从未见过这些物体。系统使用了 CNNs 来观察世界，有两个理解相关物理世界的模型。前向模型（forward model），用来预测一个动作/戳的结果，以及一个能够获取当前状况并将之映射到行动中的逆模型（inverse model）。通过一系列令人信服的视频，很明显，机器人已经学会如何相当普遍地四处移动物体。 似然推理、Dessert 类比（Dessert analogies）]]></content>
      <tags>
        <tag>conference</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RAM]]></title>
    <url>%2F2017%2F06%2F05%2FRAM%2F</url>
    <content type="text"><![CDATA[http://jacoxu.com/ramreasoning-attention-memory%E4%B8%8Eqa%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%E8%B5%84%E6%BA%90%E5%88%97%E8%A1%A8/]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLP]]></title>
    <url>%2F2017%2F06%2F05%2FNLP%2F</url>
    <content type="text"><![CDATA[http://www.iteye.com/news/31261]]></content>
  </entry>
  <entry>
    <title><![CDATA[CV]]></title>
    <url>%2F2017%2F06%2F05%2FCV%2F</url>
    <content type="text"><![CDATA[http://blog.csdn.net/Hao_Zhang_Vision/article/details/53175979?locationNum=2&amp;fps=1http://blog.csdn.net/cuixiaoxue/article/details/70138784http://blog.csdn.net/sinat_26917383/article/details/54669846]]></content>
  </entry>
  <entry>
    <title><![CDATA[attention]]></title>
    <url>%2F2017%2F06%2F05%2Fattention%2F</url>
    <content type="text"><![CDATA[http://blog.csdn.net/jdbc/article/details/52948351http://www.cnblogs.com/taojake-ML/p/6113459.html Show, Attend and Tell: Neural Image Caption Generation with Visual Attentionhttp://www.cnblogs.com/wangxiaocvpr/p/5537454.htmlhttp://blog.csdn.net/cuixiaoxue/article/details/70138784Teaching Machines to Read and Comprehendhttp://blog.csdn.net/u014300008/article/details/52763100http://chuansong.me/n/1362687251865]]></content>
  </entry>
  <entry>
    <title><![CDATA[VAE]]></title>
    <url>%2F2017%2F06%2F05%2FVAE%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[GAN]]></title>
    <url>%2F2017%2F06%2F05%2FGAN%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[image-caption]]></title>
    <url>%2F2017%2F06%2F05%2Fimage-caption%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[RNNs]]></title>
    <url>%2F2017%2F06%2F05%2FRNNs%2F</url>
    <content type="text"><![CDATA[LSTM http://colah.github.io/posts/2015-08-Understanding-LSTMs/ Tree-Structured LSTMsA generalization of the standard LSTM architecture to tree structure.Input $x_i$ regards $x_{i-1}$ as its child, then generalize one child to more. Child-Sum Tree-LSTMs UsageSince the Child-Sum Tree-LSTM unit conditions its components on the sum of child hidden states $h_k$ , it is well-suited for trees with high branching factor or whose children are unordered.For example, it is a good choice for dependency trees, where the number of dependents of a head can be highly variable.We refer to a Child-Sum Tree-LSTM applied to a dependency tree as a Dependency Tree-LSTM. N-ary Tree-LSTMs UsageIn Eq. 10, we define a parameterization of the kth child’s forget gate $f_{jk}$ that contains “off-diagonal” parameter matrices $U_{kl}^{(f)}$,$ k \not= l $. This parameterization allows for more flexible control of information propagation from child to parent.We can naturally apply Binary Tree-LSTM units to binarized constituency trees since left and right child nodes are distinguished.We refer to this application of Binary Tree-LSTMs as a Constituency Tree-LSTM.]]></content>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NMT]]></title>
    <url>%2F2017%2F06%2F05%2FNMT%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[QA]]></title>
    <url>%2F2017%2F06%2F05%2FQA%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[VQA]]></title>
    <url>%2F2017%2F06%2F05%2FVQA%2F</url>
    <content type="text"><![CDATA[http://blog.csdn.net/lsh894609937/article/details/70210541http://blog.csdn.net/mxs30443/article/details/53540649http://blog.csdn.net/zdcs/article/details/54944643https://zhuanlan.zhihu.com/p/22530291?refer=dlclass]]></content>
      <tags>
        <tag>VQA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[work_temp]]></title>
    <url>%2F2017%2F06%2F05%2Fwork-temp%2F</url>
    <content type="text"><![CDATA[整理scope配对数据整理从scope数据库找到的连配对。汇总并统计数据。 database proteins alignments_family alignments_superfamily alignments_fold scope20_2.06 7659 16194 66229 133900 scope40_2.06 13760 98558 305553 526136 scope70_2.06 21132 314039 907374 1470385 scope95_2.06 28010 1099425 2963970 4170984 scope100_2.06 69590 6060416 16688439 23215716 scopeall_2.06 216248 69574992 172157027 239906177 Tensorflow 框架下NMT分析google - seq2seqA general-purpose encoder-decoder frameworkused for the paper:Massive Exploration of Neural Machine Translation Architectures eske - seq2seq (baseline branch)replicate “Neural Machine Translation by Jointly Learning to Align and Translate” paarthneekhara - byteNet-tensorflow (need to read)replicate “Neural Machine Translation in Linear Time.” ufal - neuralmonkey NAn open-source tool for sequence learning in NLP built on TensorFlow.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Embeddings in Biological Sequences]]></title>
    <url>%2F2017%2F05%2F22%2Fbiovec%2F</url>
    <content type="text"><![CDATA[ObjectiveConstruct a distributed representation of biological sequences. Related WorkWord2vec CBOW Skip-gram Doc2vecDoc2vec-CBOW Doc2vec-DM Effect In fact, unstable. BioVec: ProtVec+GeneVec （Plos one）ObjectiveOur goal is to construct a distributed representation of biological sequences. MethodHere, a biological sequence is treated like a sentence in a text corpus while the kmers derived from the sequence are treated like words, and are given as input to the embedding algorithm. Data Preprocess NN ModelSkip-gram Model EvaluationProtein Family ClassificationTo evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families,Database：Swiss-ProtMethod: SVMResults：an average family classification accuracy of 93% ± 0.06% is obtained, outperforming existing family classification methods. Disordered Proteins Classification In addition, we use ProtVec representation to predict disordered proteins from structured proteins.Database:The DisProt database as well as FG-Nups (a database featuring thedisordered regions of nucleoporins rich with phenylalanine-glycine repeats).Method:Using support vector machine classifiers,Results:FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0% accuracy. Conclusion By only providing sequence data for various proteins into this model, accurate information about protein structure can be determined. Importantly, this model needs to be trained only once and can then be applied to extract a comprehensive set of information regarding proteins of interest. Moreover, this representation can be considered as pre-training for various applications of deep learning in bioinformatics. Seq2vec (DTMBio)ObjectiveConstructing a distributed representation of biological sequences. MethodOur algorithm is based on the doc2vec approach, which is an extension of the original word2vec algorithm. Data PreprocessTwo approach. Non-overlapping Process QWERTYQWERTY-&gt;Seq 1: QWE RTY QWE RTYSeq 2: WER TYQ WERSeq 3: ERT YQW ERT. Overlapping Process: QWERTYQWERTY-&gt;QWE WER ERT RTY TYQ YQW QWE WER ERT RTY. EvaluationProtein Classification.First, we use the protein vectors learned using seq2vec and ProtVecs as features and compare them for the task of protein classification using SVMs.Next, since distributed representations embed similar sequences in proximity to each other, we use k-nearest neighbors (kNN) to retrieve the k nearest sequences in the vector space and see how successful are we in predicting the family of a test sequence based on a majority vote.]]></content>
      <tags>
        <tag>embedding</tag>
        <tag>biological</tag>
        <tag>sequences</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo usage]]></title>
    <url>%2F2017%2F05%2F19%2Fstart%2F</url>
    <content type="text"><![CDATA[Create a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词向量 word embedding]]></title>
    <url>%2F2017%2F05%2F19%2Fword-embedding%2F</url>
    <content type="text"><![CDATA[Objective问题起源文本是符号数据,两个词只要字面不同,就难以刻画它们之间的联系,即使是“麦克风”和“话筒”这样的同义词,从字面上也难以看出这两者意思相同(语义鸿沟现象)。（符号性这个角度类似于生物序列数据）我们希望计算机可以从大规模无标注的文本数据中自动学习得到文本表示,这种表示需要包含对应语言单元(词或文档)的语义信息,同时可以直接通过这种表示度量文本之间的语义相似度。 形式化问题 1 输入：语句的集合/文章的集合输出: 语言中词的一种表示 1954年，Harris 提出分布假说(distributional hypothesis),即“上下文相似的词,其语义也相似“。（只针对词）在分布假说中,需要关注的对象有两个:词和上下文,其中最关键的是上下文的表示。 形式化问题 2 输入：词的上下文输出: 词的一种表示 神经网络模型可以使用组合方式(?)对上下文进行建模,只需线性复杂度即可对复杂的 n 元短语进行建模。神经网络模型生成的词表示通常被称为词向量(word embedding)。 词向量：一个低维的实数向量表示。通过这种表示,可以直接对词之间的相似度进行刻画。 问题形式化 输入: 词的上下文输出: 词的低维实数向量表示方法: 神经网络 Related Knowledge符号表示$w$ 表示一个词语.$w_1 , w_2 , …, w_m $表示一条由m个词语组成的语句. 语言模型语言模型可以对一段文本的概率进行估计.形式化的讲,统计语言模型的作用是为一个长度为 $m$ 的字符串确定一个概率分布 $P (w_1 , w_2 , …, w_m )$ P(w_1 , w_2 , ..., w_m ) = P (w_1 ) P (w_2 |w_1 ) \dots P (w_i | w_1 , w_2 , ..., w_{i−1} )\dots P (w_m | w_1 , w_2 , ..., w_{m−1} )$n$元(n-gram)模型如果文本的长度较长,公式右部$ P (w_i | w_1 , w_2 , \dots , w_{i−1} ) $的估算会非常困难。因此,研究者们提出使用一个简化模型:$n$ 元模型.对上述条件概率做了以下近似: P (w_i | w_1 , w_2 , ..., w_i−1 ) ≈ P (w_i | w_{i−(n−1)} , . . . , w_i−1 )在 n 元模型中,传统的方法一般采用频率计数的比例来估算 $n$ 元条件概率: P (w_i | w_{i−(n−1)} , . . . , w_{i−1} ) = \frac{count(w_{i−(n−1)} , . . . , w_{i−1} , w_i ) }{count(w_{i−(n−1)} , . . . , w_{i−1} )}其中,$count(w_{i−(n−1)} , . . . , w_{i−1} )$ 表示文本序列 $w_{i−(n−1)} , . . . , w_{i−1}$ 在语料中出现的次数。 MethodMethod 1: 神经网络语言模型(NNLM)RNNLM 利用前n个词的上文信息,预测下一个词.即,直接对$ P (w_i | w_1 , w_2 , …, w_{i−1} ) $进行建模.RNNLM是利用神经网络对条件概率建模. 网络输入输出 目标: 最大化 $P (w_{i-(n-1)/2}| w_{i−(n−1)} , . . . , w_{i−1} )$输入: 整个词序列 $w_{i−(n−1)} , . . . , w_{i−1}$输出: 目标词的分布 网络结构示意图 输入层词$w_{i−(n−1)} , . . . , w_{i−1}$的词向量的顺序拼接 x = [e(w_{i−(n−1)} ); . . . ; e(w_{i−2} ); e(w_{i−1} )]隐藏层hh= tanh(b^{(1)} + Hx)y = b ^{(2)} + W x + U h其中,其中,$φ$ 为非线性激活函数。根据公式,每一个隐藏层包含了当前词的信息以及上一个隐藏层的信息。通过这种迭代推进的方式,每个隐藏层实际上包含了此前所有上文的信息. 输出层 P (w_i | w_{i−(n−1)} , . . . , w_{i−1} ) = \frac {exp (y(w_i))}{\sum_{k=1}^{|V|}exp(y(v_k))}网络目标最大化 $\sum\limits _{w _{i−(n−1):i} ∈D}log P (w_i | w_{i−(n−1)} ,\dots, w_{i−1} )$即最大化语言模型. Method 2: 循环神经网络语言模型(RNNLM)RNNLM 利用所有的上文信息,预测下一个词.即,直接对$ P (w_i | w_1 , w_2 , …, w_{i−1} ) $进行建模.RNNLM是利用神经网络对条件概率建模. 网络输入输出 目标: 最大化 $P (w_i | w_{i−(n−1)} , . . . , w_1 )$输入: 整个词序列 $w_{i−(n−1)} , . . . , w_{i−1}$输出: 目标词的分布 网络结构示意图 输入层词$w_{i−(n−1)} , . . . , w_{i−1}$的词向量的顺序拼接 x = [e(w_{i−(n−1)} ); . . . ; e(w_{i−2} ); e(w_{i−1} )]隐藏层hh(i) = φ(e(w_i ) + w_h(i − 1))y(i) = b + uh(i)其中,其中,$φ$ 为非线性激活函数。根据公式,每一个隐藏层包含了当前词的信息以及上一个隐藏层的信息。通过这种迭代推进的方式,每个隐藏层实际上包含了此前所有上文的信息. 输出层 P (w_i | w_{i−(n−1)} , . . . , w_1 ) = \frac {exp (y(w_i))}{\sum_{k=1}^{|V|}exp(y(v_k))}网络目标最大化语言模型$\sum\limits _{w _{i−(n−1):i} ∈D}log P (w_i | w_{i−(n−1)} ,\dots, w_1 )$ Method 3: CBOW 模型该模型一方面使用一段文本的中间词作为目标词;另一方面,又以 NNLM 作为蓝本,并在其基础上做了两个简化。一、CBOW 没有隐藏层,去掉隐藏层之后,模型从神经网络结构直接转化为 log 线性结构,与 Logistic 回归一致。log 线性结构比三层神经网络结构少了一个矩阵运算,大幅度地提升了模型的训练速度。二、CBOW 去除了上下文各词的词序信息,使用上下文各词词向量的平均值,代替神经网络语言模型使用的上文各词词向量的拼接。 网络输入输出 目标: 最大化 $P (w_{i-(n-1)/2} | w_{i−(n−1)} , . . . , w_{i−1} )$输入: 整个词序列 $w_{i−(n−1)} , . . . , w_{i−1}$输出: 目标词的分布 网络结构示意图 输入层词$w_{i−(n−1)} , . . . , w_{i−1}$的词向量的组合 x = \frac{1}{n-1}\sum\limits _{w_j \in c}e(w_j)隐藏层h没有 输出层c表示上下文 P (w |c) = \frac {exp (e'(w)^Tx)}{\sum_{w' \in V}exp(e'(w')^Tx)}网络目标最大化 $\sum\limits_{(w,c)\in D}logP(w|c)$ Method 4: Skip-gram 模型与 CBOW 模型一样, Skip-gram 模型中也没有隐藏层。和 CBOW 模型不同的是,Skip-gram 模型每次从目标词 w 的上下文 c 中选择一个词,将其词向量作为模型的输入 x,也就是上下文的表示。 Skip-gram模型同样通过上下文预测目标词,对于整个语料的优化目标为最大化: \sum_{(w,c) \in D}\sum_{w_j \in c}log P (w|w_j ) 在 Skip-gram 的论文 中将模型描述成通过目标词预测上下文。由于模型需要遍历整个语料,任意一个窗口中的两个词 w_a , w_b 都需要计算 P (w_a |w_b ) + P (w_b |w_a ),因此这两种描述方式是等价的。 网络输入输出 目标: 最大化 $P (w_i | w_{i−(n−1)} , . . . , w_{i−1} )$输入: 整个词序列 $w_{i−(n−1)} , . . . , w_{i−1}$输出: 目标词的分布 网络结构示意图 输入层词$w_{i−(n−1)} , . . . , w_{i−1}$的词向量的组合 x = \frac{1}{n-1}\sum\limits _{w_j \in c}e(w_j)隐藏层h没有 输出层c表示上下文 P (w |c) = \frac {exp (e'(w)^Tx)}{\sum_{w' \in V}exp(e'(w')^Tx)}网络目标最大化 $\sum\limits _{(w,c) \in D}\sum\limits _{w_j \in c}log P (w|w_j )$ 采用手段负采样技术(negative sampling)构造负样本,构造出了一个优化目标,最大化正样本的似然,同时最小化负样本的似然。其中训练时,一个正样本可以对应多个负样本. 二次采样技术(subsampling)在大规模语料中,高频词通常就是停用词.一方面,这些高频词只能带来非常少量的语义信息.另一方面,训练高频词本身占据了大量的时间,但在迭代过程中,这些高频词的词向量变化并不大。如果词 w 在语料中的出现频率 $f (w) $大于阈值 t,则有 $P (w)$ 的概率在训练时跳过这个词。 P (w) = \frac{f (w) − t}{f (w)} - \sqrt{\frac{t}{f(w)}}模型总结 模型名称 上下文 隐含层 词序 NNLM n-gram(前n个词) 非线性变换 有词序 RNNLM 前面所有词 非线性变换 有词序 CBOW n-gram(前后n个词) 无,线性组合 无词序 NNLM n-gram(前或者后面1个词) 无 无词序 研究者进行一些对比试验,因需要引入其他知识,此处暂不列出.他们得出以下结论. 一、简单模型在小语料上整体表现更好,而复杂的模型需要更大的语料作支撑。二、对于实际的自然语言处理任务,各模型的差异不大。三、同领域的语料,语料越大效果越好。四、领域内的语料对相似领域任务的效果提升非常明显,但在领域不契合时甚至会有负面作用。五、包含的语义特征更为丰富的语料(而不只是数量),在评价语义特性的任务中,效果也更好。六、词向量的维度一般需要选择 50 维及以上,特别当衡量词向量的语言学特性时,词向量的维度越大,效果越好。 注:本文部分内容总结自来斯惟博士的毕业论文&lt;&lt;基于神经网络的词和文档语义向量表示方法研究&gt;&gt;.在此表示感谢,另附上来博士的博客地址: http://licstar.net/archives/tag/%E8%AF%8D%E5%90%91%E9%87%8F]]></content>
      <categories>
        <category>NLP</category>
        <category>summary</category>
      </categories>
      <tags>
        <tag>embedding</tag>
        <tag>neural_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习笔记（一）]]></title>
    <url>%2F2017%2F05%2F11%2Ftensorflow-study-1%2F</url>
    <content type="text"><![CDATA[TF的编程模型来源：http://blog.csdn.net/tinyzhao/article/details/52755647 计算图在TensorFlow中，算法都被表示成计算图（computational graphs）。计算图也叫数据流图,图中的节点表示操作，图中的边代表在不同操作之间的数据流动。在这样的数据流图中，有四个主要的元素概念： 操作(operations) 张量(tensors) 变量(variables) 会话(sessions) 操作(operations)数据流过节点的时候对数据进行的操作，操作包含很多种。 操作类型 例子 元素运算 Add,Mul 矩阵运算 MatMul,MatrixInverse 数值产生 Constant,Variable 神经网络单元 SoftMax,ReLU,Conv2D I/O Save,Restore 张量(tensors)图中的每个边代表数据从一个操作流到另一个操作。这些数据被表示为张量。一个张量可以看做是多维的数组或者高维的矩阵。张量本身并没有保存任何值，张量仅仅提供了访问数值的一个接口，可以看做是数值的一种引用。在TensorFlow实际使用中我们也可以发现，在run之前的张量并没有分配空间，此时的张量仅仅表示了一种数值的抽象，用来连接不同的节点，表示数据在不同操作之间的流动。TensorFlow中还提供了SparseTensor数据结构，用来表示稀疏张量。 变量(variables)可以改变数值和数据类型的节点。在实际处理时，一般把需要训练的值指定为变量。在使用变量的时候，需要指定变量的初始值，变量的大小和数据类型就是根据初始值来推断的。在构建计算图的时候，指定一个变量实际上需要增加三个节点： 实际的变量节点 一个产生初始值的操作，通常是一个常量节点 一个初始化操作，把初始值赋予到变量 如图所示，v代表的是实际的变量，i是产生初始值的节点，上面的assign节点将初始值赋予变量，assign操作以后，产生已经初始化的变量值v’。 tensorflow的基本运作方式12345678910111213#coding=utf-8import tensorflow as tf#定义占位符a = tf.placeholder(tf.int32,shape=())b = tf.placeholder(tf.int32,shape=())#构建运算图y = tf.multiply(a, b) #构造一个op节点#创建会话sess = tf.Session()#运行会话，输入数据，并计算节点，同时打印结果print(sess.run(y, feed_dict=&#123;a: 3, b: 3&#125;))# 任务完成, 关闭会话.sess.close() TF的运行流程来源：http://www.cnblogs.com/wuzhitj/p/6297734.html 概念描述Tensor在训练开始前，所有的数据都是抽象的概念1234import tensorflow as tf # 在下面所有代码中，都去掉了这一行，默认已经导入a = tf.zeros(shape=[1,2])print(a)#Tensor("zeros:0", shape=(1, 2), dtype=float32) 只有在训练过程开始后，才能获得a的实际值123sess = tf.InteractiveSession()print(sess.run(a))#[[ 0. 0.]] Variable故名思议，是变量的意思,与Tensor不同，Variable必须初始化以后才有具体的值1234567tensor = tf.zeros(shape=[1,2])variable = tf.Variable(tensor)sess = tf.InteractiveSession()# print(sess.run(variable)) # 会报错sess.run(tf.initialize_all_variables()) # 对variable进行初始化print(sess.run(variable))#[[ 0. 0.]] placeholder又叫占位符，同样是一个抽象的概念。用于表示输入输出数据的格式。告诉系统：这里有一个值/向量/矩阵，现在我没法给你具体数值，不过我正式运行的时候会补上的！例如上式中的x和y。因为没有具体数值，所以只要指定尺寸即可。12x = tf.placeholder(tf.float32,[1, 5],name='input')y = tf.placeholder(tf.float32,[None, 5],name='input') 上面有两种形式，第一种x，表示输入是一个[1,5]的横向量。而第二种形式，表示输入是一个[?,5]的矩阵。那么什么情况下会这么用呢?就是需要输入一批[1,5]的数据的时候。比如我有一批共10个数据，那我可以表示成[10,5]的矩阵。如果是一批5个，那就是[5,5]的矩阵。tensorflow会自动进行批处理 Sessionsession，也就是会话。session是抽象模型的实现者。只有实现了模型以后，才能够得到具体的值。]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
</search>